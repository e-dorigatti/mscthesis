{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import time\n",
    "from scipy.stats import probplot\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from sklearn.utils import shuffle\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, GaussianNoise, Input, PReLU, Activation, Concatenate\n",
    "from keras.initializers import VarianceScaling\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    dframe_path = 'data/cabauw/processed-full-log.csv.gz'\n",
    "    df = pd.read_csv(dframe_path, na_values='--', compression='gzip')\n",
    "\n",
    "    df = df[(df.ustar > 0.1) & (abs(df.H) > 10) & (df.wind > 1)]\n",
    "    df = df[df.ds != 201603]\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_index(dtimes, interval):\n",
    "    # returns a tuple index_above, index_below\n",
    "    # index_above[i] is the largest i\n",
    "    # such that dtimes[index_above[i]] - dtimes[i] < interval\n",
    "    # index_below[i] is the smallest i\n",
    "    # such that dtimes[i] - dtimes[index_below[i]] < interval\n",
    "    # dtimes must be already sorted!\n",
    "    index_below, index_above = np.zeros(\n",
    "        (2, len(dtimes)), dtype=np.int\n",
    "    ) - 1\n",
    "    \n",
    "    for i, x in enumerate(dtimes):\n",
    "        j = index_below[i - 1] if i > 0 else 0\n",
    "        while x - dtimes[j] > interval:\n",
    "            j += 1\n",
    "\n",
    "        index_below[i] = j\n",
    "        index_above[j] = i\n",
    "\n",
    "    last_above = index_above[0]\n",
    "    for i in range(len(dtimes)):\n",
    "        if index_above[i] < 0:\n",
    "            index_above[i] = last_above\n",
    "        else:\n",
    "            last_above = index_above[i]\n",
    "    \n",
    "    return index_above, index_below\n",
    "\n",
    "\n",
    "def compute_trend(df, columns, interval=3600):\n",
    "    df = df.sort_values('datetime')\n",
    "    for z in df.z.unique():  \n",
    "        this_level = df[df.z == z]\n",
    "        index_above, index_below = make_index(this_level.datetime.values, interval)\n",
    "\n",
    "        for col in columns:\n",
    "            val_above = this_level[col].values\n",
    "            val_below = this_level.iloc[index_below][col].values\n",
    "\n",
    "            time_above = this_level.datetime.values\n",
    "            time_below = this_level.iloc[index_below].datetime.values\n",
    "\n",
    "            trend = 3600 * (val_above - val_below) / (time_above - time_below)\n",
    "\n",
    "            df.loc[df.z == z, col + '_trend'] = trend\n",
    "\n",
    "    return df, [col + '_trend' for col in columns]\n",
    "\n",
    "\n",
    "def get_features(df, use_trend, feature_level):\n",
    "    wind_temp_levels = df.pivot_table(\n",
    "        values=['wind', 'temp'], columns='z', index=['ds', 'tt']\n",
    "    ).reset_index()\n",
    "    wind_temp_levels.columns = [\n",
    "        '%s_%d' % (a, b) if b else a\n",
    "        for a, b in wind_temp_levels.columns.values\n",
    "    ]\n",
    "\n",
    "    df = df.merge(wind_temp_levels, on=['ds', 'tt'])\n",
    "\n",
    "    feature_sets = [\n",
    "        [\n",
    "            'z', 'wind', 'temp', 'soil_temp',\n",
    "            'wind_10', 'wind_20', 'wind_40',\n",
    "            'temp_10', 'temp_20', 'temp_40',\n",
    "        ],\n",
    "        ['soilheat'],\n",
    "        ['netrad'],\n",
    "        ['rain', 'dewpoint'],\n",
    "        ['H', 'LE'],\n",
    "    ]\n",
    "\n",
    "    features = [\n",
    "        f for fset in feature_sets[:feature_level]\n",
    "        for f in fset\n",
    "    ]\n",
    "\n",
    "    if use_trend:\n",
    "        df, added_cols = compute_trend(df, [\n",
    "            f for f in features if f != 'z'\n",
    "        ])\n",
    "        features.extend(added_cols)\n",
    "\n",
    "    return df, features\n",
    "\n",
    "\n",
    "def get_train_test_data(df, features, target, n_months=12):\n",
    "    # remove feature columns with only nulls and rows with any null\n",
    "    empty_columns = df.isnull().all(axis=0)\n",
    "    keep_columns = df.columns.isin(features) & ~empty_columns\n",
    "    missing = df.loc[:, keep_columns].isnull().any(axis=1)\n",
    "    df = df[~missing]\n",
    "\n",
    "    # get random test months\n",
    "    test_ds = np.random.choice(df.ds.unique(), n_months, replace=False)\n",
    "    test_mask = df.ds.isin(test_ds)\n",
    "\n",
    "    train_x, train_y = df.loc[~test_mask, keep_columns], df.loc[~test_mask, target]\n",
    "    test_x, test_y = df.loc[test_mask, keep_columns], df.loc[test_mask, target]\n",
    "\n",
    "    mean_x, mean_y = train_x.mean(), train_y.mean()\n",
    "    std_x, std_y = train_x.std(), train_y.std()\n",
    "\n",
    "    train_x = (train_x - mean_x) /  std_x\n",
    "    test_x = (test_x - mean_x) / std_x\n",
    "    \n",
    "    assert np.all(np.isfinite(train_x))\n",
    "    \n",
    "    train_y = (train_y - mean_y) / std_y\n",
    "    test_y = (test_y - mean_y) / std_y\n",
    "\n",
    "    features = keep_columns.index.values[keep_columns.values]\n",
    "    return features, train_x, train_y, test_x, test_y, mean_y, std_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_denormalized_mse(std_y):\n",
    "    def denormalized_mse(y_true, y_pred):\n",
    "        # model is trained with normalized data, but we want\n",
    "        # mse on not normalized data to compare with MOST\n",
    "        mse = K.mean(K.square(y_true - y_pred), axis=-1)\n",
    "        return mse * std_y**2\n",
    "    return denormalized_mse\n",
    "\n",
    "\n",
    "\n",
    "def build_model(sizes, std_y=1):\n",
    "    # every element in sizes specifies a layer\n",
    "    #   negative number: skip connection of -n layers\n",
    "    #                    successive skips are aggregated\n",
    "    #                    into a single layer\n",
    "    #   0<n<1: dropout with pkeep=n\n",
    "    #   >1 fully connected then prelu\n",
    "    layers = [Input(shape=(sizes[0],))]\n",
    "    i = 1\n",
    "    while i < len(sizes):\n",
    "        num = sizes[i]\n",
    "        if num < 0:\n",
    "            skip = [layers[-1]]\n",
    "            while i < len(sizes) and sizes[i] < 0:\n",
    "                skip.append(layers[sizes[i] - 1])\n",
    "                i += 1\n",
    "            layer = Concatenate()(skip)\n",
    "            i -= 1\n",
    "        elif num < 1:\n",
    "            layer = Dropout(num)(layers[-1])\n",
    "        else:\n",
    "            layer = PReLU()(\n",
    "                Dense(num, kernel_initializer=VarianceScaling(2, 'fan_in'))(\n",
    "                    layers[-1]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        layers.append(layer)\n",
    "        i += 1\n",
    "\n",
    "    layers.append(Dense(1)(layers[-1]))\n",
    "\n",
    "    opt = Adam(lr=0.001)\n",
    "    model = Model(inputs=layers[0], outputs=layers[-1])\n",
    "    model.compile(loss='mse', optimizer=opt, metrics=[compute_denormalized_mse(std_y)])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(network, use_trend, feature_level, batch_size=1024, verbose=2):\n",
    "    ddf, features = get_features(df, use_trend, feature_level)\n",
    "    features, train_x, train_y, test_x, test_y, mean_y, std_y = get_train_test_data(ddf, features, 'phi_m')\n",
    "\n",
    "    K.clear_session()  # https://stackoverflow.com/q/35114376/521776\n",
    "    model = build_model([len(features)] + network, std_y=std_y)\n",
    "\n",
    "    dtime = datetime.datetime.utcnow().isoformat().replace('-', '').replace(':', '').replace('T', '-')[:-7]\n",
    "    logdir = 'dev/logs/%s-tren:%s-features:%s-batchsize:%s-nparam:%s/' % (\n",
    "        dtime, use_trend, feature_level, batch_size, model.count_params()\n",
    "    )\n",
    "\n",
    "    if verbose > 0:\n",
    "        print('Saving to', logdir)\n",
    "\n",
    "    callbacks = [\n",
    "        ReduceLROnPlateau(factor=0.1, verbose=verbose, min_lr=1e-6, patience=10, monitor='loss'),\n",
    "        ModelCheckpoint(logdir + 'weights-w.{epoch:04d}-{val_loss:.4f}.hdf5',\n",
    "                        verbose=verbose, save_best_only=True),\n",
    "        TensorBoard(logdir, write_graph=True, write_grads=True, histogram_freq=0),\n",
    "        EarlyStopping(min_delta=0.0001, patience=50),\n",
    "    ]\n",
    "\n",
    "    hist = model.fit(\n",
    "        train_x, train_y,\n",
    "        batch_size=batch_size,\n",
    "        epochs=1000,\n",
    "        verbose=verbose,\n",
    "        shuffle=True,\n",
    "        callbacks=callbacks,\n",
    "        validation_data=(test_x, test_y)\n",
    "    )\n",
    "\n",
    "    return hist, logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/ipykernel_launcher.py:43: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bsize 1024 fset 1 trend True network 0 best 0.339911 @ 84 - logdir dev/logs/20180324-194659-tren:True-features:1-batchsize:1024-nparam:53802/\n",
      "bsize 1024 fset 1 trend True network 1 best 0.307802 @ 62 - logdir dev/logs/20180324-202848-tren:True-features:1-batchsize:1024-nparam:54890/\n",
      "bsize 1024 fset 2 trend True network 0 best 0.302378 @ 69 - logdir dev/logs/20180324-210314-tren:True-features:2-batchsize:1024-nparam:54314/\n",
      "bsize 1024 fset 2 trend True network 1 best 0.304889 @ 71 - logdir dev/logs/20180324-213826-tren:True-features:2-batchsize:1024-nparam:55402/\n",
      "bsize 1024 fset 3 trend True network 0 best 0.292065 @ 63 - logdir dev/logs/20180324-221607-tren:True-features:3-batchsize:1024-nparam:54826/\n",
      "bsize 1024 fset 3 trend True network 1 best 0.341736 @ 29 - logdir dev/logs/20180324-224938-tren:True-features:3-batchsize:1024-nparam:55914/\n",
      "bsize 1024 fset 4 trend True network 0 best 0.306433 @ 59 - logdir dev/logs/20180324-231314-tren:True-features:4-batchsize:1024-nparam:55850/\n",
      "bsize 1024 fset 4 trend True network 1 best 0.244598 @ 76 - logdir dev/logs/20180324-234608-tren:True-features:4-batchsize:1024-nparam:56938/\n",
      "bsize 1024 fset 5 trend True network 0 best 0.277206 @ 83 - logdir dev/logs/20180325-002603-tren:True-features:5-batchsize:1024-nparam:56874/\n",
      "bsize 1024 fset 5 trend True network 1 best 0.252249 @ 65 - logdir dev/logs/20180325-010642-tren:True-features:5-batchsize:1024-nparam:57962/\n",
      "bsize 1024 fset 1 trend False network 0 best 0.391637 @ 31 - logdir dev/logs/20180325-014307-tren:False-features:1-batchsize:1024-nparam:51498/\n",
      "bsize 1024 fset 1 trend False network 1 best 0.346011 @ 23 - logdir dev/logs/20180325-020658-tren:False-features:1-batchsize:1024-nparam:52586/\n",
      "bsize 1024 fset 2 trend False network 0 best 0.324900 @ 11 - logdir dev/logs/20180325-022943-tren:False-features:2-batchsize:1024-nparam:51754/\n",
      "bsize 1024 fset 2 trend False network 1 best 0.289282 @ 18 - logdir dev/logs/20180325-024759-tren:False-features:2-batchsize:1024-nparam:52842/\n",
      "bsize 1024 fset 3 trend False network 0 best 0.320737 @ 20 - logdir dev/logs/20180325-030910-tren:False-features:3-batchsize:1024-nparam:52010/\n",
      "bsize 1024 fset 3 trend False network 1 best 0.313763 @ 33 - logdir dev/logs/20180325-033006-tren:False-features:3-batchsize:1024-nparam:53098/\n",
      "bsize 1024 fset 4 trend False network 0 best 0.323527 @ 18 - logdir dev/logs/20180325-035606-tren:False-features:4-batchsize:1024-nparam:52522/\n",
      "bsize 1024 fset 4 trend False network 1 best 0.285501 @ 63 - logdir dev/logs/20180325-041625-tren:False-features:4-batchsize:1024-nparam:53610/\n",
      "bsize 1024 fset 5 trend False network 0 best 0.276768 @ 69 - logdir dev/logs/20180325-045141-tren:False-features:5-batchsize:1024-nparam:53034/\n",
      "bsize 1024 fset 5 trend False network 1 best 0.238060 @ 12 - logdir dev/logs/20180325-052718-tren:False-features:5-batchsize:1024-nparam:54122/\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    #[64, 32, 16, 8, 4, 2, 1],\n",
    "    #[128, 64, 32, 16, 8, 4, 2, 1],\n",
    "    #[256, 0.5, 128, 64, 32, 16, 8, 4, 2, 1],\n",
    "    [256, 0.5, 128, 64, 64, 32, 16, 8, 4, 2, 1], \n",
    "    [256, 0.5, 128, 64, 64, 32, 32, 16, 8, 4, 2, 1], \n",
    "]\n",
    "\n",
    "\n",
    "bsize = 1024\n",
    "for trend in [True, False]:\n",
    "    for fset in [1, 2, 3, 4, 5]:\n",
    "        for i, network in enumerate(models):\n",
    "            hist, logdir = run_experiment(network, trend, fset, bsize, verbose=0)\n",
    "            best = min(hist.history['val_denormalized_mse'])\n",
    "            best_at = np.argmin(hist.history['val_denormalized_mse'])\n",
    "\n",
    "            print('bsize %s fset %s trend %s network %s best %f @ %s - logdir %s' % (\n",
    "                bsize, fset, trend, i, best, best_at, logdir\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
