{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from hkfold import HKFold\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from keras.layers import PReLU\n",
    "from sklearn.utils import shuffle\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, GaussianNoise\n",
    "from keras.models import load_model\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dframe_path = 'data/cabauw/processed.csv.gz'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(dframe_path, na_values='--')\n",
    "except UnicodeDecodeError:\n",
    "    df = pd.read_csv(dframe_path, na_values='--', compression='gzip')\n",
    "\n",
    "\n",
    "df = df[(df.ustar > 0.1) & (abs(df.H) > 10) & (df.wind > 1)]\n",
    "df = df[df.ds != 201603]\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'dewpoint', 'spec_hum', 'rel_hum', 'press', 'rain',\n",
    "    'temp', 'soil_temp', 'z', 'air_dens', 'virtual_temp',\n",
    "    'wind',\n",
    "    #'H', 'LE',\n",
    "    #'lval',\n",
    "    #'zL', 'ustar', \n",
    "]\n",
    "\n",
    "target = 'phi_m'\n",
    "\n",
    "test_ds = np.random.choice(df.ds.unique(), 18, replace=False)\n",
    "test_mask = df.ds.isin(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with 393120, testing with 50244\n"
     ]
    }
   ],
   "source": [
    "def get_data(mask, columns, merge_z_values):\n",
    "    if merge_z_values:\n",
    "        return np.vstack([\n",
    "            np.hstack(g[columns].values)\n",
    "            for _, g in df[mask].groupby(['ds', 'tt'])\n",
    "        ])\n",
    "    else:\n",
    "        return df[mask][columns]\n",
    "\n",
    "\n",
    "merge_z = True\n",
    "train_x, train_y = get_data(~test_mask, features, merge_z), get_data(~test_mask, target, merge_z)\n",
    "test_x, test_y = get_data(test_mask, features, merge_z), get_data(test_mask, target, merge_z)\n",
    "\n",
    "#train_x, train_y = shuffle(train_x, train_y)\n",
    "print('training with %d, testing with %d' % (len(train_x), len(test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_x, mean_y = train_x.mean(), train_y.mean()\n",
    "std_x, std_y = train_x.std(), train_y.std()\n",
    "\n",
    "norm_train_x = (train_x - mean_x) /  std_x\n",
    "norm_test_x = (test_x - mean_x) / std_x\n",
    "\n",
    "norm_train_y = (train_y - mean_y) / std_y\n",
    "norm_test_y = (test_y - mean_y) / std_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_101 (Dense)            (None, 1024)              46080     \n",
      "_________________________________________________________________\n",
      "p_re_lu_84 (PReLU)           (None, 1024)              1024      \n",
      "_________________________________________________________________\n",
      "dense_102 (Dense)            (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "p_re_lu_85 (PReLU)           (None, 1024)              1024      \n",
      "_________________________________________________________________\n",
      "dense_103 (Dense)            (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "p_re_lu_86 (PReLU)           (None, 256)               256       \n",
      "_________________________________________________________________\n",
      "dense_104 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "p_re_lu_87 (PReLU)           (None, 256)               256       \n",
      "_________________________________________________________________\n",
      "dense_105 (Dense)            (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "p_re_lu_88 (PReLU)           (None, 64)                64        \n",
      "_________________________________________________________________\n",
      "dense_106 (Dense)            (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "p_re_lu_89 (PReLU)           (None, 64)                64        \n",
      "_________________________________________________________________\n",
      "dense_107 (Dense)            (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "p_re_lu_90 (PReLU)           (None, 32)                32        \n",
      "_________________________________________________________________\n",
      "dense_108 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "p_re_lu_91 (PReLU)           (None, 16)                16        \n",
      "_________________________________________________________________\n",
      "dense_109 (Dense)            (None, 4)                 68        \n",
      "=================================================================\n",
      "Total params: 1,449,892\n",
      "Trainable params: 1,449,892\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(1024, input_shape=(len(train_x[0]),)), PReLU(),\n",
    "    Dense(1024), PReLU(),\n",
    "    Dense(256), PReLU(),\n",
    "    Dense(256), PReLU(),\n",
    "    Dense(64), PReLU(),\n",
    "    Dense(64), PReLU(),\n",
    "    Dense(32), PReLU(),\n",
    "    Dense(16), PReLU(),\n",
    "    Dense(len(train_y[0]))\n",
    "])\n",
    "\n",
    "opt = RMSprop(lr=0.001)\n",
    "model.compile(loss='mse', optimizer=opt)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 393120 samples, validate on 50244 samples\n",
      "Epoch 1/250\n",
      "392192/393120 [============================>.] - ETA: 0s - loss: 0.7086\n",
      "Epoch 00001: val_loss improved from inf to 0.66939, saving model to dev/models/weights-w.0001-0.6694.hdf5\n",
      "393120/393120 [==============================] - 11s 27us/step - loss: 0.7085 - val_loss: 0.6694\n",
      "Epoch 2/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.6744\n",
      "Epoch 00002: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.6745 - val_loss: 0.8222\n",
      "Epoch 3/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.6754\n",
      "Epoch 00003: val_loss improved from 0.66939 to 0.59848, saving model to dev/models/weights-w.0003-0.5985.hdf5\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.6753 - val_loss: 0.5985\n",
      "Epoch 4/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.6622\n",
      "Epoch 00004: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.6620 - val_loss: 0.6267\n",
      "Epoch 5/250\n",
      "390144/393120 [============================>.] - ETA: 0s - loss: 0.6694\n",
      "Epoch 00005: val_loss improved from 0.59848 to 0.59478, saving model to dev/models/weights-w.0005-0.5948.hdf5\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.6693 - val_loss: 0.5948\n",
      "Epoch 6/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.6424\n",
      "Epoch 00006: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.6429 - val_loss: 0.5978\n",
      "Epoch 7/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.6414\n",
      "Epoch 00007: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.6421 - val_loss: 0.6138\n",
      "Epoch 8/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.6393\n",
      "Epoch 00008: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.6391 - val_loss: 0.6433\n",
      "Epoch 9/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.6309\n",
      "Epoch 00009: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.6311 - val_loss: 0.6513\n",
      "Epoch 10/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.6350\n",
      "Epoch 00010: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.6347 - val_loss: 0.7099\n",
      "Epoch 11/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.6324\n",
      "Epoch 00011: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.6323 - val_loss: 0.6254\n",
      "Epoch 12/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.6303\n",
      "Epoch 00012: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.6300 - val_loss: 0.7993\n",
      "Epoch 13/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.6291\n",
      "Epoch 00013: val_loss improved from 0.59478 to 0.58860, saving model to dev/models/weights-w.0013-0.5886.hdf5\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.6289 - val_loss: 0.5886\n",
      "Epoch 14/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.6208\n",
      "Epoch 00014: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.6215 - val_loss: 0.6332\n",
      "Epoch 15/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.6110\n",
      "Epoch 00015: val_loss improved from 0.58860 to 0.55757, saving model to dev/models/weights-w.0015-0.5576.hdf5\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.6109 - val_loss: 0.5576\n",
      "Epoch 16/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.6131\n",
      "Epoch 00016: val_loss improved from 0.55757 to 0.55163, saving model to dev/models/weights-w.0016-0.5516.hdf5\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.6128 - val_loss: 0.5516\n",
      "Epoch 17/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.6025\n",
      "Epoch 00017: val_loss improved from 0.55163 to 0.53924, saving model to dev/models/weights-w.0017-0.5392.hdf5\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.6024 - val_loss: 0.5392\n",
      "Epoch 18/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.5951\n",
      "Epoch 00018: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.5949 - val_loss: 0.5507\n",
      "Epoch 19/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.5948\n",
      "Epoch 00019: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.5947 - val_loss: 0.6568\n",
      "Epoch 20/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.5890\n",
      "Epoch 00020: val_loss improved from 0.53924 to 0.52540, saving model to dev/models/weights-w.0020-0.5254.hdf5\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.5892 - val_loss: 0.5254\n",
      "Epoch 21/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.5919\n",
      "Epoch 00021: val_loss improved from 0.52540 to 0.51426, saving model to dev/models/weights-w.0021-0.5143.hdf5\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.5917 - val_loss: 0.5143\n",
      "Epoch 22/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.5828\n",
      "Epoch 00022: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.5827 - val_loss: 0.7480\n",
      "Epoch 23/250\n",
      "392192/393120 [============================>.] - ETA: 0s - loss: 0.5732\n",
      "Epoch 00023: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.5732 - val_loss: 0.7311\n",
      "Epoch 24/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.5714\n",
      "Epoch 00024: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 22us/step - loss: 0.5711 - val_loss: 0.5336\n",
      "Epoch 25/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.5676\n",
      "Epoch 00025: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.5675 - val_loss: 0.5369\n",
      "Epoch 26/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.5648\n",
      "Epoch 00026: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 22us/step - loss: 0.5645 - val_loss: 0.5469\n",
      "Epoch 27/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.5680\n",
      "Epoch 00027: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 22us/step - loss: 0.5679 - val_loss: 0.5158\n",
      "Epoch 28/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.5615\n",
      "Epoch 00028: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 22us/step - loss: 0.5613 - val_loss: 0.5799\n",
      "Epoch 29/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.5573\n",
      "Epoch 00029: val_loss improved from 0.51426 to 0.50293, saving model to dev/models/weights-w.0029-0.5029.hdf5\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.5573 - val_loss: 0.5029\n",
      "Epoch 30/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.5560\n",
      "Epoch 00030: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 22us/step - loss: 0.5569 - val_loss: 0.5396\n",
      "Epoch 31/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.5558\n",
      "Epoch 00031: val_loss improved from 0.50293 to 0.48715, saving model to dev/models/weights-w.0031-0.4872.hdf5\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.5559 - val_loss: 0.4872\n",
      "Epoch 32/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.5496\n",
      "Epoch 00032: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 22us/step - loss: 0.5515 - val_loss: 0.6420\n",
      "Epoch 33/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.5473\n",
      "Epoch 00033: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 22us/step - loss: 0.5471 - val_loss: 0.5061\n",
      "Epoch 34/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.5489\n",
      "Epoch 00034: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 22us/step - loss: 0.5484 - val_loss: 0.5156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.5462\n",
      "Epoch 00035: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 22us/step - loss: 0.5479 - val_loss: 0.5806\n",
      "Epoch 36/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.5456\n",
      "Epoch 00036: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 22us/step - loss: 0.5455 - val_loss: 0.5000\n",
      "Epoch 37/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.5355\n",
      "Epoch 00037: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 22us/step - loss: 0.5361 - val_loss: 0.5313\n",
      "Epoch 38/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.5325\n",
      "Epoch 00038: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.5328 - val_loss: 0.5180\n",
      "Epoch 39/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.5368\n",
      "Epoch 00039: val_loss improved from 0.48715 to 0.48324, saving model to dev/models/weights-w.0039-0.4832.hdf5\n",
      "393120/393120 [==============================] - 9s 22us/step - loss: 0.5365 - val_loss: 0.4832\n",
      "Epoch 40/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.5323\n",
      "Epoch 00040: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 22us/step - loss: 0.5324 - val_loss: 0.7368\n",
      "Epoch 41/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.5297\n",
      "Epoch 00041: val_loss improved from 0.48324 to 0.43702, saving model to dev/models/weights-w.0041-0.4370.hdf5\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.5293 - val_loss: 0.4370\n",
      "Epoch 42/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.5272\n",
      "Epoch 00042: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 22us/step - loss: 0.5279 - val_loss: 0.7759\n",
      "Epoch 43/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.5301\n",
      "Epoch 00043: val_loss improved from 0.43702 to 0.43395, saving model to dev/models/weights-w.0043-0.4340.hdf5\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.5295 - val_loss: 0.4340\n",
      "Epoch 44/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.5217\n",
      "Epoch 00044: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 22us/step - loss: 0.5215 - val_loss: 0.5096\n",
      "Epoch 45/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.5203\n",
      "Epoch 00045: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 22us/step - loss: 0.5201 - val_loss: 0.4422\n",
      "Epoch 46/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.5177\n",
      "Epoch 00046: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.5174 - val_loss: 0.5238\n",
      "Epoch 47/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.5153\n",
      "Epoch 00047: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.5161 - val_loss: 0.5512\n",
      "Epoch 48/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.5239\n",
      "Epoch 00048: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 22us/step - loss: 0.5235 - val_loss: 0.4864\n",
      "Epoch 49/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.5156\n",
      "Epoch 00049: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 22us/step - loss: 0.5158 - val_loss: 0.5616\n",
      "Epoch 50/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.5081\n",
      "Epoch 00050: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 22us/step - loss: 0.5091 - val_loss: 0.5568\n",
      "Epoch 51/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.5087\n",
      "Epoch 00051: val_loss improved from 0.43395 to 0.41928, saving model to dev/models/weights-w.0051-0.4193.hdf5\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.5086 - val_loss: 0.4193\n",
      "Epoch 52/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.5040\n",
      "Epoch 00052: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.5038 - val_loss: 0.4694\n",
      "Epoch 53/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.5011\n",
      "Epoch 00053: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.5007 - val_loss: 0.4301\n",
      "Epoch 54/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.5306\n",
      "Epoch 00054: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 22us/step - loss: 0.5305 - val_loss: 0.4892\n",
      "Epoch 55/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.5355\n",
      "Epoch 00055: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 22us/step - loss: 0.5353 - val_loss: 0.6962\n",
      "Epoch 56/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.5194\n",
      "Epoch 00056: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 22us/step - loss: 0.5189 - val_loss: 0.5041\n",
      "Epoch 57/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.5118\n",
      "Epoch 00057: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 22us/step - loss: 0.5118 - val_loss: 0.4748\n",
      "Epoch 58/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.5030\n",
      "Epoch 00058: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.5025 - val_loss: 0.4250\n",
      "Epoch 59/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.4975\n",
      "Epoch 00059: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.4977 - val_loss: 0.4584\n",
      "Epoch 60/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.4948\n",
      "Epoch 00060: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 22us/step - loss: 0.4948 - val_loss: 0.4276\n",
      "Epoch 61/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.4901\n",
      "Epoch 00061: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 22us/step - loss: 0.4899 - val_loss: 0.4971\n",
      "Epoch 62/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.4885\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\n",
      "Epoch 00062: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 24us/step - loss: 0.4886 - val_loss: 0.6544\n",
      "Epoch 63/250\n",
      "390144/393120 [============================>.] - ETA: 0s - loss: 0.3632\n",
      "Epoch 00063: val_loss improved from 0.41928 to 0.37489, saving model to dev/models/weights-w.0063-0.3749.hdf5\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.3631 - val_loss: 0.3749\n",
      "Epoch 64/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.3414\n",
      "Epoch 00064: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.3414 - val_loss: 0.4020\n",
      "Epoch 65/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.3319\n",
      "Epoch 00065: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.3319 - val_loss: 0.4050\n",
      "Epoch 66/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.3255\n",
      "Epoch 00066: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.3254 - val_loss: 0.3826\n",
      "Epoch 67/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.3187\n",
      "Epoch 00067: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 22us/step - loss: 0.3188 - val_loss: 0.4217\n",
      "Epoch 68/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.3133\n",
      "Epoch 00068: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 22us/step - loss: 0.3135 - val_loss: 0.4070\n",
      "Epoch 69/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.3091\n",
      "Epoch 00069: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 22us/step - loss: 0.3091 - val_loss: 0.4507\n",
      "Epoch 70/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.3049\n",
      "Epoch 00070: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 22us/step - loss: 0.3052 - val_loss: 0.4504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.3011\n",
      "Epoch 00071: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.3015 - val_loss: 0.4069\n",
      "Epoch 72/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.2982\n",
      "Epoch 00072: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 22us/step - loss: 0.2982 - val_loss: 0.4124\n",
      "Epoch 73/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.2948\n",
      "Epoch 00073: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 22us/step - loss: 0.2948 - val_loss: 0.4362\n",
      "Epoch 74/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.2911\n",
      "Epoch 00074: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "\n",
      "Epoch 00074: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.2914 - val_loss: 0.4260\n",
      "Epoch 75/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.2499\n",
      "Epoch 00075: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.2499 - val_loss: 0.4712\n",
      "Epoch 76/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.2441\n",
      "Epoch 00076: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.2441 - val_loss: 0.4632\n",
      "Epoch 77/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.2402\n",
      "Epoch 00077: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.2402 - val_loss: 0.4609\n",
      "Epoch 78/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.2367\n",
      "Epoch 00078: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.2367 - val_loss: 0.4838\n",
      "Epoch 79/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.2335\n",
      "Epoch 00079: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 22us/step - loss: 0.2335 - val_loss: 0.4791\n",
      "Epoch 80/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.2308\n",
      "Epoch 00080: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 22us/step - loss: 0.2309 - val_loss: 0.4571\n",
      "Epoch 81/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.2281\n",
      "Epoch 00081: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 22us/step - loss: 0.2281 - val_loss: 0.4716\n",
      "Epoch 82/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.2256\n",
      "Epoch 00082: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.2256 - val_loss: 0.4627\n",
      "Epoch 83/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.2230\n",
      "Epoch 00083: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.2230 - val_loss: 0.4706\n",
      "Epoch 84/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.2207\n",
      "Epoch 00084: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "\n",
      "Epoch 00084: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.2206 - val_loss: 0.4726\n",
      "Epoch 85/250\n",
      "390144/393120 [============================>.] - ETA: 0s - loss: 0.2129\n",
      "Epoch 00085: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.2129 - val_loss: 0.4835\n",
      "Epoch 86/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.2119\n",
      "Epoch 00086: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.2119 - val_loss: 0.4807\n",
      "Epoch 87/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.2111\n",
      "Epoch 00087: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.2111 - val_loss: 0.4748\n",
      "Epoch 88/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.2103\n",
      "Epoch 00088: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.2103 - val_loss: 0.4857\n",
      "Epoch 89/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.2096\n",
      "Epoch 00089: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.2096 - val_loss: 0.4874\n",
      "Epoch 90/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.2089\n",
      "Epoch 00090: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.2088 - val_loss: 0.4899\n",
      "Epoch 91/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.2081\n",
      "Epoch 00091: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.2081 - val_loss: 0.4888\n",
      "Epoch 92/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.2074\n",
      "Epoch 00092: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.2073 - val_loss: 0.4927\n",
      "Epoch 93/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.2065\n",
      "Epoch 00093: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.2066 - val_loss: 0.4903\n",
      "Epoch 94/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.2059\n",
      "Epoch 00094: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "\n",
      "Epoch 00094: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.2059 - val_loss: 0.4831\n",
      "Epoch 95/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.2047\n",
      "Epoch 00095: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.2046 - val_loss: 0.4868\n",
      "Epoch 96/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.2044\n",
      "Epoch 00096: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.2044 - val_loss: 0.4870\n",
      "Epoch 97/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.2042\n",
      "Epoch 00097: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.2043 - val_loss: 0.4862\n",
      "Epoch 98/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.2041\n",
      "Epoch 00098: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.2041 - val_loss: 0.4865\n",
      "Epoch 99/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.2040\n",
      "Epoch 00099: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.2039 - val_loss: 0.4835\n",
      "Epoch 100/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.2039\n",
      "Epoch 00100: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.2038 - val_loss: 0.4830\n",
      "Epoch 101/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.2037\n",
      "Epoch 00101: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.2036 - val_loss: 0.4869\n",
      "Epoch 102/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.2035\n",
      "Epoch 00102: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.2035 - val_loss: 0.4861\n",
      "Epoch 103/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.2033\n",
      "Epoch 00103: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.2033 - val_loss: 0.4854\n",
      "Epoch 104/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.2032\n",
      "Epoch 00104: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "\n",
      "Epoch 00104: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 22us/step - loss: 0.2031 - val_loss: 0.4855\n",
      "Epoch 105/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.2030\n",
      "Epoch 00105: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.2029 - val_loss: 0.4843\n",
      "Epoch 106/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.2028\n",
      "Epoch 00106: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.2028 - val_loss: 0.4854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.2027\n",
      "Epoch 00107: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.2028 - val_loss: 0.4846\n",
      "Epoch 108/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.2027\n",
      "Epoch 00108: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.2026 - val_loss: 0.4877\n",
      "Epoch 109/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.2024\n",
      "Epoch 00109: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.2026 - val_loss: 0.4848\n",
      "Epoch 110/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.2024\n",
      "Epoch 00110: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.2025 - val_loss: 0.4871\n",
      "Epoch 111/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.2023\n",
      "Epoch 00111: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.2024 - val_loss: 0.4873\n",
      "Epoch 112/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.2023\n",
      "Epoch 00112: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.2023 - val_loss: 0.4842\n",
      "Epoch 113/250\n",
      "391168/393120 [============================>.] - ETA: 0s - loss: 0.2022\n",
      "Epoch 00113: val_loss did not improve\n",
      "393120/393120 [==============================] - 9s 23us/step - loss: 0.2022 - val_loss: 0.4872\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(\n",
    "    norm_train_x, norm_train_y,\n",
    "    batch_size=1024, epochs=250,\n",
    "    shuffle=True,\n",
    "    callbacks=[\n",
    "        ReduceLROnPlateau(factor=0.2, verbose=1, min_lr=1e-6),\n",
    "        ModelCheckpoint('dev/models/weights-w.{epoch:04d}-{val_loss:.4f}.hdf5',\n",
    "                        verbose=1, save_best_only=True),\n",
    "        TensorBoard('dev/logs/', write_graph=False, write_grads=True),\n",
    "        EarlyStopping(min_delta=0.0001, patience=50),\n",
    "    ],\n",
    "    validation_data=(norm_test_x, norm_test_y)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_preds(ypred, ytrue):\n",
    "    minn = max(min(ypred), min(ytrue))\n",
    "    maxx = min(max(ypred), max(ytrue))\n",
    "    \n",
    "    plt.scatter(ytrue, ypred, s=2)\n",
    "    plt.plot([minn, maxx], [minn, maxx], 'r--')\n",
    "    plt.xlabel('True')\n",
    "    plt.ylabel('Predicted')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8156982a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance 0.6182242210763684\n",
      "Mean absolute error 0.6173660203515089\n",
      "Mean squared error 1.018016735709469\n",
      "Median absolute error 0.3532730259941834\n",
      "R2 score 0.6180234215571526\n",
      "Mean absolute percent error 74.75959819831358\n"
     ]
    }
   ],
   "source": [
    "best_model = load_model('dev/models/weights-w.0053-0.3767.hdf5')\n",
    "y_pred = best_model.predict(norm_test_x)\n",
    "\n",
    "# denormalize\n",
    "test_deno = norm_test_y * std_y + mean_y\n",
    "pred_deno = y_pred * std_y + mean_y\n",
    "\n",
    "if merge_z:\n",
    "    pred_deno = [y for ss in pred_deno for y in ss]\n",
    "    test_deno = [y for ss in test_deno for y in ss]\n",
    "\n",
    "plot_preds(pred_deno, test_deno)\n",
    "\n",
    "print('Explained variance', metrics.explained_variance_score(test_deno, pred_deno))\n",
    "print('Mean absolute error', metrics.mean_absolute_error(test_deno, pred_deno))\n",
    "print('Mean squared error', metrics.mean_squared_error(test_deno, pred_deno))\n",
    "print('Median absolute error', metrics.median_absolute_error(test_deno, pred_deno))\n",
    "print('R2 score', metrics.r2_score(test_deno, pred_deno))\n",
    "mape = 100 * sum(abs((yt - yp) / yt) for yt, yp in zip(test_deno, pred_deno)) / len(test_deno)\n",
    "print('Mean absolute percent error', mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
