{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import time\n",
    "from scipy.stats import probplot\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from sklearn.utils import shuffle\n",
    "import random\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import (Dense, Dropout, BatchNormalization, LSTM,\n",
    "                          GaussianNoise, Input, PReLU, Activation,\n",
    "                          Concatenate, GRU, Masking, TimeDistributed,\n",
    "                          Conv2D, MaxPooling2D, Flatten)\n",
    "from keras.initializers import VarianceScaling\n",
    "from keras import regularizers \n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    dframe_path = 'data/cabauw/processed-full-log.csv.gz'\n",
    "    df = pd.read_csv(dframe_path, na_values='--', compression='gzip')\n",
    "\n",
    "    df = df[(df.ustar > 0.1) & (abs(df.H) > 10) & (df.wind > 1)]\n",
    "    df = df[df.ds != 201603]\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RollingMeanStd:\n",
    "    # http://people.ds.cam.ac.uk/fanf2/hermes/doc/antiforgery/stats.pdf\n",
    "    def __init__(self):\n",
    "        self.n = 0\n",
    "        self.mu = self.s = None\n",
    "    \n",
    "    def add(self, x):\n",
    "        self.n += 1\n",
    "        if self.n == 1:\n",
    "            self.mu = self.s = x\n",
    "            return\n",
    "\n",
    "        old_mu = self.mu\n",
    "        old_s = self.s\n",
    "        \n",
    "        self.mu = old_mu + (x - old_mu) / self.n\n",
    "        self.s = old_s + (x - old_mu) * (x - self.mu)\n",
    "    \n",
    "    @property\n",
    "    def count(self):\n",
    "        return self.n\n",
    "\n",
    "    @property\n",
    "    def rolling_mean(self):\n",
    "        return self.mu\n",
    "    \n",
    "    @property\n",
    "    def rolling_std(self):\n",
    "        return np.sqrt(self.s / self.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_temp_levels = df.pivot_table(\n",
    "    values=['wind', 'temp'], columns='z', index=['ds', 'tt']\n",
    ").reset_index()\n",
    "wind_temp_levels.columns = [\n",
    "    '%s_%d' % (a, b) if b else a\n",
    "    for a, b in wind_temp_levels.columns.values\n",
    "]\n",
    "\n",
    "ddf = df.merge(wind_temp_levels, on=['ds', 'tt'])\n",
    "\n",
    "feature_sets = [\n",
    "    [\n",
    "        'z', 'wind', 'temp', 'soil_temp',\n",
    "        'wind_10', 'wind_20', 'wind_40',\n",
    "        'temp_10', 'temp_20', 'temp_40',\n",
    "    ],\n",
    "    ['soilheat'],\n",
    "    ['netrad'],\n",
    "    ['rain', 'dewpoint'],\n",
    "]\n",
    "\n",
    "all_features = [f for fset in feature_sets for f in fset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51989 3\n",
      "103978 7\n",
      "155967 11\n",
      "207956 15\n",
      "259945 19\n",
      "311934 23\n",
      "363923 27\n",
      "415912 31\n",
      "467901 35\n",
      "519890 39\n",
      "571879 43\n",
      "623868 47\n",
      "675857 51\n",
      "727846 55\n",
      "779835 59\n",
      "831824 63\n",
      "883813 67\n",
      "935802 71\n",
      "987791 75\n",
      "1039780 79\n",
      "1091769 83\n",
      "1143758 87\n",
      "1195747 91\n",
      "1247736 95\n",
      "1299725 99\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "window_size = 1 * 60 * 60\n",
    "\n",
    "queues = {z: deque() for z in ddf.z.unique()}\n",
    "normalizer = RollingMeanStd()\n",
    "samples_x, samples_y = [], []\n",
    "train_mask = []\n",
    "nonas = ddf.dropna().sort_values('datetime')\n",
    "test_ds = np.random.choice(nonas.ds.unique(), 20, replace=False)\n",
    "test_mask = nonas.ds.isin(test_ds)\n",
    "max_len = 0\n",
    "\n",
    "for i, row in enumerate(nonas.itertuples()):\n",
    "    qq = queues[row.z]\n",
    "    qq.append(row)\n",
    "    while qq and row.datetime - qq[0].datetime > window_size:\n",
    "        qq.popleft()\n",
    "\n",
    "    sample = []\n",
    "    for obs in qq:\n",
    "        ss = [getattr(obs, ff) for ff in all_features]\n",
    "        ss.append(row.datetime - obs.datetime)\n",
    "        sample.append(ss)\n",
    "    \n",
    "    max_len = max(max_len, len(sample))\n",
    "    if not test_mask.iloc[i]:\n",
    "        normalizer.add(np.array(ss[:-1]))\n",
    "\n",
    "    samples_x.append(np.array(sample))\n",
    "    samples_y.append(row.phi_m)\n",
    "    \n",
    "    if (i + 1) % int(len(nonas) / 25) == 0:\n",
    "        print(i + 1, int(100 * (i + 1) / len(nonas)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 7, 15), (32, 1))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu_x = normalizer.rolling_mean\n",
    "std_x = normalizer.rolling_std\n",
    "\n",
    "mu_y = np.mean(np.extract(test_mask, samples_y))\n",
    "std_y = np.std(np.extract(test_mask, samples_y))\n",
    "\n",
    "def make_batches(all_samples, epochs, batch_size, pad_to,\n",
    "                 num_features, shuffle=True, adaptive_width=True):\n",
    "    ep = 0\n",
    "\n",
    "    batch_x = np.zeros((batch_size, pad_to, num_features))\n",
    "    batch_y = np.zeros((batch_size, 1))\n",
    "\n",
    "    while epochs < 0 or ep < epochs:\n",
    "        if shuffle:\n",
    "            random.shuffle(all_samples)\n",
    "\n",
    "        ep += 1\n",
    "        max_len = 0\n",
    "        batch_x[:, :, -1] = -1\n",
    "\n",
    "        for i, (sx, sy) in enumerate(all_samples):\n",
    "            max_len = max(max_len, sx.shape[0])\n",
    "\n",
    "            batch_x[i % batch_size, -sx.shape[0]:, :-1] = (sx[:, :-1] - mu_x) / std_x\n",
    "            batch_x[i % batch_size, -sx.shape[0]:, -1] = sx[:, -1] / window_size\n",
    "            batch_y[i % batch_size] = (sy - mu_y) / std_y\n",
    "\n",
    "            assert np.all(np.isfinite(sx)), i\n",
    "            assert np.all(np.isfinite(sy)), i\n",
    "\n",
    "            if (i + 1) % batch_size == 0:\n",
    "                if adaptive_width:\n",
    "                    yield batch_x[:, -max_len:], batch_y\n",
    "                else:\n",
    "                    yield batch_x, batch_y\n",
    "\n",
    "                max_len = 0\n",
    "                batch_x[:, :, -1] = -1\n",
    "\n",
    "        remaining = (i + 1) % batch_size\n",
    "        if remaining > 0:\n",
    "            if adaptive_width:\n",
    "                yield batch_x[:remaining, -max_len:], batch_y[:remaining]\n",
    "            else:\n",
    "                yield batch_x[:remaining, :], batch_y[:remaining, :]\n",
    "\n",
    "\n",
    "bx, by = next(make_batches(list(zip(samples_x, samples_y)), -1,\n",
    "                           32, max_len, 15, adaptive_width=True))\n",
    "bx.shape, by.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98269"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_denormalized_mse(std_y):\n",
    "    def denorm_mse(y_true, y_pred):\n",
    "        # model is trained with normalized data, but we want\n",
    "        # mse on not normalized data to compare with MOST\n",
    "        mse = K.mean(K.square(y_true - y_pred), axis=-1)\n",
    "        return mse * std_y**2\n",
    "    return denorm_mse\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    layer_kwargs = {\n",
    "        'kernel_regularizer': regularizers.l2(1e-6),\n",
    "        'activity_regularizer': regularizers.l2(1e-6),\n",
    "        'activation': None\n",
    "    }\n",
    "\n",
    "    act = lambda: PReLU()\n",
    "\n",
    "    model = Sequential([\n",
    "        #GaussianNoise(0.05, input_shape=(None, 15)),\n",
    "        GRU(128, input_shape=(None, 15), **layer_kwargs),\n",
    "        act(),\n",
    "        Dense(128, **layer_kwargs), act(),\n",
    "        Dense(128, **layer_kwargs), act(),\n",
    "        Dense(64, **layer_kwargs), act(),\n",
    "        Dense(16, **layer_kwargs), act(),\n",
    "        Dense(8, **layer_kwargs), act(),\n",
    "        Dense(4, **layer_kwargs), act(),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    opt = Adam(clipnorm=10)\n",
    "    model.compile(optimizer=opt, loss='mse',\n",
    "                  metrics=[compute_denormalized_mse(std_y)])\n",
    "    return model\n",
    "\n",
    "\n",
    "build_model().count_params()\n",
    "#build_model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "run_on_gpu = True\n",
    "\n",
    "train_samples = list(zip(np.extract(~test_mask, samples_x),\n",
    "                         np.extract(~test_mask, samples_y)))\n",
    "#train_samples = random.sample(train_samples, 100000)\n",
    "train_steps = int(len(train_samples) / batch_size) + 1\n",
    "print('%d training samples, %d batches per epoch' % (len(train_samples), train_steps))\n",
    "\n",
    "\n",
    "val_samples = list(zip(np.extract(test_mask, samples_x),\n",
    "                       np.extract(test_mask, samples_y)))\n",
    "#val_samples = random.sample(val_samples, 25000)\n",
    "val_steps = int(len(val_samples) / batch_size) + 1\n",
    "print('%d val. samples, %d batches per val.' % (len(val_samples), val_steps))\n",
    "\n",
    "\n",
    "exec_id = (datetime.datetime\n",
    "    .utcnow()\n",
    "    .isoformat()\n",
    "    .replace('-', '')\n",
    "    .replace(':', '')\n",
    "    .replace('T', '-')\n",
    ")[:-7]\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    with tf.Session(config=tf.ConfigProto(\n",
    "        device_count={'GPU': int(run_on_gpu)}\n",
    "    )).as_default():\n",
    "        \n",
    "        model = build_model()\n",
    "    \n",
    "        train_data = make_batches(train_samples, -1, batch_size, max_len, 15)\n",
    "        val_data = make_batches(val_samples, -1, batch_size, max_len, 15)\n",
    "        model.fit_generator(\n",
    "            generator=train_data,\n",
    "            steps_per_epoch=train_steps,\n",
    "            epochs=500,\n",
    "            validation_data=val_data,\n",
    "            validation_steps=val_steps,\n",
    "            verbose=1,\n",
    "            callbacks=[\n",
    "                ReduceLROnPlateau(epsilon=0.001, verbose=1, min_lr=1e-6),\n",
    "                TensorBoard('dev/logs/rnn-%s' % exec_id)\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plain dense fc networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_index(dtimes, interval):\n",
    "    # returns a tuple index_above, index_below\n",
    "    # index_above[i] is the largest i\n",
    "    # such that dtimes[index_above[i]] - dtimes[i] < interval\n",
    "    # index_below[i] is the smallest i\n",
    "    # such that dtimes[i] - dtimes[index_below[i]] < interval\n",
    "    # dtimes must be already sorted!\n",
    "    index_below, index_above = np.zeros(\n",
    "        (2, len(dtimes)), dtype=np.int\n",
    "    ) - 1\n",
    "    \n",
    "    for i, x in enumerate(dtimes):\n",
    "        j = index_below[i - 1] if i > 0 else 0\n",
    "        while x - dtimes[j] > interval:\n",
    "            j += 1\n",
    "\n",
    "        index_below[i] = j\n",
    "        index_above[j] = i\n",
    "\n",
    "    last_above = index_above[0]\n",
    "    for i in range(len(dtimes)):\n",
    "        if index_above[i] < 0:\n",
    "            index_above[i] = last_above\n",
    "        else:\n",
    "            last_above = index_above[i]\n",
    "    \n",
    "    return index_above, index_below\n",
    "\n",
    "\n",
    "def compute_trend(df, columns, interval=3600):\n",
    "    df = df.sort_values('datetime')\n",
    "    for z in df.z.unique():  \n",
    "        this_level = df[df.z == z]\n",
    "        index_above, index_below = make_index(this_level.datetime.values, interval)\n",
    "\n",
    "        for col in columns:\n",
    "            val_above = this_level[col].values\n",
    "            val_below = this_level.iloc[index_below][col].values\n",
    "\n",
    "            time_above = this_level.datetime.values\n",
    "            time_below = this_level.iloc[index_below].datetime.values\n",
    "\n",
    "            trend = 3600 * (val_above - val_below) / (time_above - time_below)\n",
    "\n",
    "            df.loc[df.z == z, col + '_trend'] = trend\n",
    "\n",
    "    return df, [col + '_trend' for col in columns]\n",
    "\n",
    "\n",
    "def get_features(df, use_trend, feature_level):\n",
    "    wind_temp_levels = df.pivot_table(\n",
    "        values=['wind', 'temp'], columns='z', index=['ds', 'tt']\n",
    "    ).reset_index()\n",
    "    wind_temp_levels.columns = [\n",
    "        '%s_%d' % (a, b) if b else a\n",
    "        for a, b in wind_temp_levels.columns.values\n",
    "    ]\n",
    "\n",
    "    df = df.merge(wind_temp_levels, on=['ds', 'tt'])\n",
    "\n",
    "    feature_sets = [\n",
    "        [\n",
    "            'z', 'wind', 'temp', 'soil_temp',\n",
    "            'wind_10', 'wind_20', 'wind_40',\n",
    "            'temp_10', 'temp_20', 'temp_40',\n",
    "        ],\n",
    "        ['soilheat'],\n",
    "        ['netrad'],\n",
    "        ['rain', 'dewpoint'],\n",
    "        ['H', 'LE'],\n",
    "    ]\n",
    "\n",
    "    features = [\n",
    "        f for fset in feature_sets[:feature_level]\n",
    "        for f in fset\n",
    "    ]\n",
    "\n",
    "    if use_trend:\n",
    "        df, added_cols = compute_trend(df, [\n",
    "            f for f in features if f != 'z'\n",
    "        ])\n",
    "        features.extend(added_cols)\n",
    "        \n",
    "    return df, features\n",
    "\n",
    "\n",
    "def get_train_test_data_my_random_months(df, features, target, n_months=18):\n",
    "    test_ds = np.random.choice(df.ds.unique(), n_months, replace=False)\n",
    "    test_mask = df.ds.isin(test_ds)\n",
    "\n",
    "    train_x, train_y = df.loc[~test_mask, features], df.loc[~test_mask, target]\n",
    "    test_x, test_y = df.loc[test_mask, features], df.loc[test_mask, target]\n",
    "\n",
    "    mean_x, mean_y = train_x.mean(), train_y.mean()\n",
    "    std_x, std_y = train_x.std(), train_y.std()\n",
    "\n",
    "    train_x = (train_x - mean_x) / std_x\n",
    "    test_x = (test_x - mean_x) / std_x\n",
    "    \n",
    "    assert np.all(np.isfinite(train_x))\n",
    "    assert np.all(np.isfinite(test_x))\n",
    "    \n",
    "    train_y = (train_y - mean_y) / std_y\n",
    "    test_y = (test_y - mean_y) / std_y\n",
    "\n",
    "    return features, train_x, train_y, test_x, test_y, mean_y, std_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttributeKFold:\n",
    "    ''' k-fold cross validator splitting on a particular attribute\n",
    "        so that all samples with a given value are either in the train or test set\n",
    "\n",
    "        attribute value for each sample is given in the constructor, so that\n",
    "        the attribute itself need not be in the features for the model\n",
    "    '''\n",
    "    def __init__(self, cv, attr):\n",
    "        self.cv, self.attr = cv, attr\n",
    "\n",
    "    def get_n_splits(self, *args, **kwargs):\n",
    "        return self.cv.get_n_splits(*args, **kwargs)\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        vals = self.attr.unique()\n",
    "        for train_idx, test_idx in self.cv.split(vals):\n",
    "            train_mask = self.attr.isin(vals[train_idx])\n",
    "            test_mask = self.attr.isin(vals[test_idx])\n",
    "\n",
    "            X = np.argwhere(train_mask).reshape(-1)\n",
    "            y = np.argwhere(test_mask).reshape(-1)\n",
    "            \n",
    "            assert np.all(np.isfinite(X))\n",
    "            assert np.all(np.isfinite(y))\n",
    "            \n",
    "            yield X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_denormalized_mse(std_y):\n",
    "    def denormalized_mse(y_true, y_pred):\n",
    "        # model is trained with normalized data, but we want\n",
    "        # mse on not normalized data to compare with MOST\n",
    "        mse = K.mean(K.square(y_true - y_pred), axis=-1)\n",
    "        return mse * std_y**2\n",
    "    return denormalized_mse\n",
    "\n",
    "\n",
    "def orthonormal_regularizer(regu):\n",
    "    # eq. 3 in https://arxiv.org/pdf/1703.01827.pdf\n",
    "    def compute(weight_matrix):\n",
    "        rows, cols = weight_matrix.shape\n",
    "        wtw = K.dot(K.transpose(weight_matrix), weight_matrix)\n",
    "        return regu * K.sum((wtw - K.eye(cols.value))**2) / 2\n",
    "    return compute\n",
    "\n",
    "\n",
    "def build_model(sizes, std_y=1):\n",
    "    # every element in sizes specifies a layer\n",
    "    #   negative number: skip connection of -n layers\n",
    "    #                    successive skips are aggregated\n",
    "    #                    into a single layer\n",
    "    #   0<n<1: dropout with pkeep=n\n",
    "    #   >1 fully connected then prelu\n",
    "    layers = [Input(shape=(sizes[0],)),]\n",
    "    #layers.append(GaussianNoise(0.01)(layers[-1]))\n",
    "    i = 1\n",
    "    while i < len(sizes):\n",
    "        num = sizes[i]\n",
    "        if num < 0:\n",
    "            skip = [layers[-1]]\n",
    "            while i < len(sizes) and sizes[i] < 0:\n",
    "                skip.append(layers[sizes[i] - 1])\n",
    "                i += 1\n",
    "            layer = Concatenate()(skip)\n",
    "            i -= 1\n",
    "        elif num < 1:\n",
    "            layer = Dropout(num)(layers[-1])\n",
    "        else:\n",
    "            layer = PReLU()(\n",
    "                Dense(num, kernel_initializer=VarianceScaling(2, 'fan_in'))(\n",
    "                    layers[-1]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        layers.append(layer)\n",
    "        i += 1\n",
    "\n",
    "    layers.append(Dense(1)(layers[-1]))\n",
    "\n",
    "    opt = Adam(lr=0.001)\n",
    "    model = Model(inputs=layers[0], outputs=layers[-1])\n",
    "    model.compile(loss='mse', optimizer=opt, metrics=[compute_denormalized_mse(std_y)])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(network, use_trend, feature_level, batch_size=1024, verbose=2, most_only=False):\n",
    "    ddf, features = get_features(df.dropna(), use_trend, feature_level)\n",
    "    if most_only:\n",
    "        ddf = ddf[(ddf.zL > -2) & (ddf.zL < 1)]\n",
    "    features, train_x, train_y, test_x, test_y, mean_y, std_y = get_train_test_data_my_random_months(\n",
    "        ddf, features, 'phi_m'\n",
    "    )\n",
    "\n",
    "    K.clear_session()  # https://stackoverflow.com/q/35114376/521776\n",
    "    model = build_model([len(features)] + network, std_y=std_y)\n",
    "\n",
    "    dtime = datetime.datetime.utcnow().isoformat().replace('-', '').replace(':', '').replace('T', '-')[:-7]\n",
    "    logdir = 'dev/logs/%s-tren%s-features%s-batchsize%s-nparam%s/' % (\n",
    "        dtime, use_trend, feature_level, batch_size, model.count_params()\n",
    "    )\n",
    "\n",
    "    if verbose > 0:\n",
    "        print('Saving to', logdir)\n",
    "\n",
    "    callbacks = [\n",
    "        ReduceLROnPlateau(factor=0.1, verbose=verbose, min_lr=1e-6, patience=10),\n",
    "        ModelCheckpoint(logdir + 'best.hdf5', verbose=verbose, save_best_only=True),\n",
    "        TensorBoard(logdir, write_graph=True, write_grads=True, histogram_freq=0),\n",
    "        EarlyStopping(min_delta=0.0001, patience=25),\n",
    "    ]\n",
    "\n",
    "    hist = model.fit(\n",
    "        train_x, train_y,\n",
    "        batch_size=batch_size,\n",
    "        epochs=1000,\n",
    "        verbose=verbose,\n",
    "        shuffle=True,\n",
    "        callbacks=callbacks,\n",
    "        validation_data=(test_x, test_y)\n",
    "    )\n",
    "\n",
    "    return hist, logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_data_by_index(df, features, target, train_idx, test_idx, normalize):\n",
    "    train_x, train_y = df.iloc[train_idx][features], df.iloc[train_idx][target]\n",
    "    test_x, test_y = df.iloc[test_idx][features], df.iloc[test_idx][target]\n",
    "\n",
    "    if normalize:\n",
    "        mean_x, std_x = train_x.mean(), train_x.std()\n",
    "        train_x = (train_x - mean_x) / std_x\n",
    "        test_x = (test_x - mean_x) / std_x\n",
    "\n",
    "        mean_y, std_y = train_y.mean(), train_y.std()\n",
    "        train_y = (train_y - mean_y) / std_y\n",
    "        test_y = (test_y - mean_y) / std_y\n",
    "    else:\n",
    "        mean_y, std_y = 0, 1\n",
    "\n",
    "    return train_x, train_y, test_x, test_y, mean_y, std_y\n",
    "\n",
    "\n",
    "def run_cv(network, use_trend, feature_level, most_only, batch_size=1024, cv_folds=5, verbose=0):\n",
    "    ddf, features = get_features(df.dropna(), use_trend, feature_level)\n",
    "    finite = np.isfinite(ddf[features]).all(axis=1)\n",
    "    ddf = ddf.loc[finite]\n",
    "    if most_only:\n",
    "        ddf = ddf[(ddf.zL > -2) & (ddf.zL < 1)]\n",
    "\n",
    "    cv = AttributeKFold(KFold(cv_folds, shuffle=True), ddf.ds)\n",
    "    results = []\n",
    "    for cv_idx, (train_idx, test_idx) in enumerate(cv.split(ddf.ds)):\n",
    "\n",
    "        # prepare data\n",
    "        train_x, train_y, test_x, test_y, mean_y, std_y = get_train_test_data_by_index(\n",
    "            ddf, features, 'phi_m', train_idx, test_idx, normalize=True\n",
    "        )\n",
    "\n",
    "        K.clear_session()  # https://stackoverflow.com/q/35114376/521776\n",
    "        model = build_model([len(features)] + network, std_y=std_y)\n",
    "        dtime = datetime.datetime.utcnow().isoformat().replace('-', '').replace(':', '').replace('T', '-')[:-7]\n",
    "        logdir = 'dev/logs/%s-tren%s-features%s-batchsize%s-nparam%s-cv%d/' % (\n",
    "            dtime, use_trend, feature_level, batch_size, model.count_params(), cv_idx\n",
    "        )\n",
    "        save_to = logdir + 'best.hdf5'\n",
    "        if verbose > 0:\n",
    "            print('Saving to', logdir)\n",
    "\n",
    "        # fit to train data\n",
    "        hist = model.fit(\n",
    "            train_x, train_y,\n",
    "            batch_size=batch_size,\n",
    "            epochs=500,\n",
    "            verbose=verbose,\n",
    "            shuffle=True,\n",
    "            callbacks=[\n",
    "                ReduceLROnPlateau(factor=0.1, verbose=verbose, min_lr=1e-6, patience=10),\n",
    "                ModelCheckpoint(save_to, verbose=verbose, save_best_only=True),\n",
    "                TensorBoard(logdir, write_graph=True, write_grads=True, histogram_freq=0),\n",
    "                EarlyStopping(min_delta=0.0001, patience=25),\n",
    "            ],\n",
    "            validation_data=(test_x, test_y)\n",
    "        )\n",
    "        \n",
    "        # evaluate on test data\n",
    "        best = load_model(logdir + 'best.hdf5', custom_objects={\n",
    "            'denormalized_mse': compute_denormalized_mse(std_y)\n",
    "        })\n",
    "\n",
    "        y_pred = best.predict(test_x)\n",
    "        y_pred = y_pred * std_y + mean_y\n",
    "        test_y = test_y * std_y + mean_y\n",
    "        \n",
    "        y_pred = y_pred.reshape(-1)\n",
    "        test_y = test_y.values.reshape(-1)\n",
    "\n",
    "        results.append((\n",
    "            metrics.explained_variance_score(test_y, y_pred),\n",
    "            metrics.mean_absolute_error(test_y, y_pred),\n",
    "            metrics.mean_squared_error(test_y, y_pred),\n",
    "            metrics.median_absolute_error(test_y, y_pred),\n",
    "            metrics.r2_score(test_y, y_pred),\n",
    "            np.mean(np.abs((test_y - y_pred) / test_y)) * 100,\n",
    "        ))\n",
    "    \n",
    "    return pd.DataFrame(results, columns=[\n",
    "        'explained_variance', 'mean_absolute_error', 'mean_squared_error',\n",
    "        'median_absolute_error', 'r2_score', 'mean_absolute_percent_error'\n",
    "    ])\n",
    "\n",
    "\n",
    "def test_setting(use_trend, feature_level, most_only, batch_size=1024, cv_folds=10, verbose=0):\n",
    "    # test all models on the given setting\n",
    "    models = [\n",
    "        [128, 64, 32, 16, 8, 4, 2, 1],\n",
    "        [256, 0.5, 128, 64, 32, 16, 8, 4, 2, 1],\n",
    "        [256, 0.5, 128, 64, 64, 32, 16, 8, 4, 2, 1], \n",
    "        [256, 0.5, 128, 64, 64, 32, 32, 16, 8, 4, 2, 1],\n",
    "        [512, 0.5, 256, 0.5, 128, 64, 32, 16, 8, 4, 2, 1],\n",
    "        [256, 0.5, 128, 64, 64, 32, 32, 16, 8, 4, 2, 1],\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    for i, mod in enumerate(models):\n",
    "        res = run_cv(\n",
    "            mod, use_trend, feature_level, most_only, batch_size, cv_folds, verbose\n",
    "        )\n",
    "        print('----  model', i)\n",
    "        print(res.describe().T)\n",
    "        results.append(res)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/ipykernel_launcher.py:43: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----  model 0\n",
      "                             count       mean        std        min  \\\n",
      "explained_variance            10.0   0.845168   0.007878   0.836138   \n",
      "mean_absolute_error           10.0   0.223719   0.009633   0.209812   \n",
      "mean_squared_error            10.0   0.128021   0.010448   0.114250   \n",
      "median_absolute_error         10.0   0.139679   0.006043   0.131263   \n",
      "r2_score                      10.0   0.845037   0.007892   0.836013   \n",
      "mean_absolute_percent_error   10.0  66.382704  33.606492  43.173563   \n",
      "\n",
      "                                   25%        50%        75%         max  \n",
      "explained_variance            0.838149   0.843280   0.852672    0.855499  \n",
      "mean_absolute_error           0.216687   0.223827   0.230371    0.237424  \n",
      "mean_squared_error            0.119943   0.126748   0.133647    0.144310  \n",
      "median_absolute_error         0.134649   0.138898   0.145076    0.147642  \n",
      "r2_score                      0.837851   0.843267   0.852616    0.855380  \n",
      "mean_absolute_percent_error  46.180267  57.525448  65.353390  155.455487  \n",
      "----  model 1\n",
      "                             count       mean        std        min  \\\n",
      "explained_variance            10.0   0.849341   0.008675   0.837494   \n",
      "mean_absolute_error           10.0   0.223451   0.011967   0.202291   \n",
      "mean_squared_error            10.0   0.124951   0.013737   0.102776   \n",
      "median_absolute_error         10.0   0.142705   0.008627   0.129971   \n",
      "r2_score                      10.0   0.848895   0.008900   0.837316   \n",
      "mean_absolute_percent_error   10.0  74.169490  40.042720  39.615708   \n",
      "\n",
      "                                   25%        50%        75%         max  \n",
      "explained_variance            0.840806   0.851612   0.856238    0.860060  \n",
      "mean_absolute_error           0.214451   0.224468   0.232795    0.240164  \n",
      "mean_squared_error            0.114622   0.126659   0.133399    0.148540  \n",
      "median_absolute_error         0.137114   0.141694   0.147309    0.159905  \n",
      "r2_score                      0.839451   0.851342   0.856229    0.859164  \n",
      "mean_absolute_percent_error  48.647798  60.498757  83.026402  174.804220  \n",
      "----  model 2\n",
      "                             count       mean        std        min  \\\n",
      "explained_variance            10.0   0.847128   0.007022   0.835482   \n",
      "mean_absolute_error           10.0   0.225377   0.009479   0.212866   \n",
      "mean_squared_error            10.0   0.126653   0.009636   0.113132   \n",
      "median_absolute_error         10.0   0.145031   0.008707   0.135036   \n",
      "r2_score                      10.0   0.846787   0.007323   0.834582   \n",
      "mean_absolute_percent_error   10.0  70.316401  31.162344  41.907996   \n",
      "\n",
      "                                   25%        50%        75%         max  \n",
      "explained_variance            0.842521   0.848086   0.851120    0.857525  \n",
      "mean_absolute_error           0.218237   0.224545   0.229613    0.243249  \n",
      "mean_squared_error            0.120923   0.126739   0.130747    0.143580  \n",
      "median_absolute_error         0.138639   0.140918   0.152453    0.160479  \n",
      "r2_score                      0.842106   0.847788   0.850996    0.857523  \n",
      "mean_absolute_percent_error  56.546353  58.030542  68.631505  149.525413  \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Could not operate array([1.21437722e+01, 2.68556354e+00, 6.13142094e+00, 5.47967716e+00,\n       2.34921918e+00, 2.54741347e+00, 2.75082854e+00, 6.16887242e+00,\n       6.12081704e+00, 6.07938917e+00, 1.27679213e+01, 1.60938388e+02,\n       1.30745856e-01, 5.60341478e+00, 4.70466667e+01, 1.10642087e+02,\n       1.33562232e+00, 7.89776570e-01, 2.82331789e-01, 1.24140715e+00,\n       1.33764925e+00, 1.43687907e+00, 8.07513164e-01, 7.85138730e-01,\n       7.63359860e-01, 4.75344300e+00, 8.41352168e+01, 2.21128149e-01,\n       9.35291275e-01, 2.83899059e+01, 6.27276580e+01]) with block values ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, func, other, errors, try_cast, mgr)\u001b[0m\n\u001b[1;32m   1376\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1377\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(other)\u001b[0m\n\u001b[1;32m   1345\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1346\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mna_op\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1202\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpressions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr_rep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0meval_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1203\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(op, op_str, a, b, use_numexpr, **eval_kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muse_numexpr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0meval_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36m_evaluate_standard\u001b[0;34m(op, op_str, a, b, **eval_kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-451a9f5d3760>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0muse_trend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfeature_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmost_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-22-e7334dd3fea9>\u001b[0m in \u001b[0;36mtest_setting\u001b[0;34m(use_trend, feature_level, most_only, batch_size, cv_folds, verbose)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         res = run_cv(\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_trend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmost_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_folds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         )\n\u001b[1;32m    104\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'----  model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-e7334dd3fea9>\u001b[0m in \u001b[0;36mrun_cv\u001b[0;34m(network, use_trend, feature_level, most_only, batch_size, cv_folds, verbose)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# prepare data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         train_x, train_y, test_x, test_y, mean_y, std_y = get_train_test_data_by_index(\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mddf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'phi_m'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         )\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-e7334dd3fea9>\u001b[0m in \u001b[0;36mget_train_test_data_by_index\u001b[0;34m(df, features, target, train_idx, test_idx, normalize)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mmean_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mtrain_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmean_x\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mstd_x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mtest_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmean_x\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mstd_x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(self, other, axis, level, fill_value)\u001b[0m\n\u001b[1;32m   1260\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1262\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_series\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1263\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfill_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_combine_series\u001b[0;34m(self, other, func, fill_value, axis, level, try_cast)\u001b[0m\n\u001b[1;32m   3942\u001b[0m         return self._combine_series_infer(other, func, level=level,\n\u001b[1;32m   3943\u001b[0m                                           \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3944\u001b[0;31m                                           try_cast=try_cast)\n\u001b[0m\u001b[1;32m   3945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3946\u001b[0m     def _combine_series_infer(self, other, func, level=None,\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_combine_series_infer\u001b[0;34m(self, other, func, level, fill_value, try_cast)\u001b[0m\n\u001b[1;32m   3956\u001b[0m         return self._combine_match_columns(other, func, level=level,\n\u001b[1;32m   3957\u001b[0m                                            \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3958\u001b[0;31m                                            try_cast=try_cast)\n\u001b[0m\u001b[1;32m   3959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3960\u001b[0m     def _combine_match_index(self, other, func, level=None,\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_combine_match_columns\u001b[0;34m(self, other, func, level, fill_value, try_cast)\u001b[0m\n\u001b[1;32m   3979\u001b[0m         new_data = left._data.eval(func=func, other=right,\n\u001b[1;32m   3980\u001b[0m                                    \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3981\u001b[0;31m                                    try_cast=try_cast)\n\u001b[0m\u001b[1;32m   3982\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   3433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3434\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3435\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3437\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mquantile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, axes, filter, do_integrity_check, consolidate, **kwargs)\u001b[0m\n\u001b[1;32m   3327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3328\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mgr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3329\u001b[0;31m             \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3330\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, func, other, errors, try_cast, mgr)\u001b[0m\n\u001b[1;32m   1382\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdetail\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m         \u001b[0;31m# technically a broadcast error in numpy can 'work' by returning a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mhandle_error\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1365\u001b[0m                 \u001b[0;31m# The 'detail' variable is defined in outer scope.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m                 raise TypeError('Could not operate %s with block values %s' %\n\u001b[0;32m-> 1367\u001b[0;31m                                 (repr(other), str(detail)))  # noqa\n\u001b[0m\u001b[1;32m   1368\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m                 \u001b[0;31m# return the values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Could not operate array([1.21437722e+01, 2.68556354e+00, 6.13142094e+00, 5.47967716e+00,\n       2.34921918e+00, 2.54741347e+00, 2.75082854e+00, 6.16887242e+00,\n       6.12081704e+00, 6.07938917e+00, 1.27679213e+01, 1.60938388e+02,\n       1.30745856e-01, 5.60341478e+00, 4.70466667e+01, 1.10642087e+02,\n       1.33562232e+00, 7.89776570e-01, 2.82331789e-01, 1.24140715e+00,\n       1.33764925e+00, 1.43687907e+00, 8.07513164e-01, 7.85138730e-01,\n       7.63359860e-01, 4.75344300e+00, 8.41352168e+01, 2.21128149e-01,\n       9.35291275e-01, 2.83899059e+01, 6.27276580e+01]) with block values "
     ]
    }
   ],
   "source": [
    "rt5m = test_setting(\n",
    "    use_trend=True,\n",
    "    feature_level=5,\n",
    "    most_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
