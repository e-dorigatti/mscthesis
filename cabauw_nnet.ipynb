{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import time\n",
    "from scipy.stats import probplot\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from sklearn.utils import shuffle\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, GaussianNoise, Input, PReLU, Activation, Concatenate\n",
    "from keras.initializers import VarianceScaling\n",
    "from keras import regularizers \n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    dframe_path = 'data/cabauw/processed-full-log.csv.gz'\n",
    "    df = pd.read_csv(dframe_path, na_values='--', compression='gzip')\n",
    "\n",
    "    df = df[(df.ustar > 0.1) & (abs(df.H) > 10) & (df.wind > 1)]\n",
    "    df = df[df.ds != 201603]\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_index(dtimes, interval):\n",
    "    # returns a tuple index_above, index_below\n",
    "    # index_above[i] is the largest i\n",
    "    # such that dtimes[index_above[i]] - dtimes[i] < interval\n",
    "    # index_below[i] is the smallest i\n",
    "    # such that dtimes[i] - dtimes[index_below[i]] < interval\n",
    "    # dtimes must be already sorted!\n",
    "    index_below, index_above = np.zeros(\n",
    "        (2, len(dtimes)), dtype=np.int\n",
    "    ) - 1\n",
    "    \n",
    "    for i, x in enumerate(dtimes):\n",
    "        j = index_below[i - 1] if i > 0 else 0\n",
    "        while x - dtimes[j] > interval:\n",
    "            j += 1\n",
    "\n",
    "        index_below[i] = j\n",
    "        index_above[j] = i\n",
    "\n",
    "    last_above = index_above[0]\n",
    "    for i in range(len(dtimes)):\n",
    "        if index_above[i] < 0:\n",
    "            index_above[i] = last_above\n",
    "        else:\n",
    "            last_above = index_above[i]\n",
    "    \n",
    "    return index_above, index_below\n",
    "\n",
    "\n",
    "def compute_trend(df, columns, interval=3600):\n",
    "    df = df.sort_values('datetime')\n",
    "    for z in df.z.unique():  \n",
    "        this_level = df[df.z == z]\n",
    "        index_above, index_below = make_index(this_level.datetime.values, interval)\n",
    "\n",
    "        for col in columns:\n",
    "            val_above = this_level[col].values\n",
    "            val_below = this_level.iloc[index_below][col].values\n",
    "\n",
    "            time_above = this_level.datetime.values\n",
    "            time_below = this_level.iloc[index_below].datetime.values\n",
    "\n",
    "            trend = 3600 * (val_above - val_below) / (time_above - time_below)\n",
    "\n",
    "            df.loc[df.z == z, col + '_trend'] = trend\n",
    "\n",
    "    return df, [col + '_trend' for col in columns]\n",
    "\n",
    "\n",
    "def get_features(df, use_trend, feature_level):\n",
    "    wind_temp_levels = df.pivot_table(\n",
    "        values=['wind', 'temp'], columns='z', index=['ds', 'tt']\n",
    "    ).reset_index()\n",
    "    wind_temp_levels.columns = [\n",
    "        '%s_%d' % (a, b) if b else a\n",
    "        for a, b in wind_temp_levels.columns.values\n",
    "    ]\n",
    "\n",
    "    df = df.merge(wind_temp_levels, on=['ds', 'tt'])\n",
    "\n",
    "    feature_sets = [\n",
    "        [\n",
    "            'z', 'wind', 'temp', 'soil_temp',\n",
    "            'wind_10', 'wind_20', 'wind_40',\n",
    "            'temp_10', 'temp_20', 'temp_40',\n",
    "        ],\n",
    "        ['soilheat'],\n",
    "        ['netrad'],\n",
    "        ['rain', 'dewpoint'],\n",
    "        ['H', 'LE'],\n",
    "    ]\n",
    "\n",
    "    features = [\n",
    "        f for fset in feature_sets[:feature_level]\n",
    "        for f in fset\n",
    "    ]\n",
    "\n",
    "    if use_trend:\n",
    "        df, added_cols = compute_trend(df, [\n",
    "            f for f in features if f != 'z'\n",
    "        ])\n",
    "        features.extend(added_cols)\n",
    "\n",
    "    return df, features\n",
    "\n",
    "\n",
    "def get_train_test_data(df, features, target, n_months=12):\n",
    "    # remove feature columns with only nulls and rows with any null\n",
    "    empty_columns = df.isnull().all(axis=0)\n",
    "    keep_columns = df.columns.isin(features) & ~empty_columns\n",
    "    missing = df.loc[:, keep_columns].isnull().any(axis=1)\n",
    "    df = df[~missing]\n",
    "\n",
    "    # get random test months\n",
    "    test_ds = np.random.choice(df.ds.unique(), n_months, replace=False)\n",
    "    test_mask = df.ds.isin(test_ds)\n",
    "\n",
    "    train_x, train_y = df.loc[~test_mask, keep_columns], df.loc[~test_mask, target]\n",
    "    test_x, test_y = df.loc[test_mask, keep_columns], df.loc[test_mask, target]\n",
    "\n",
    "    mean_x, mean_y = train_x.mean(), train_y.mean()\n",
    "    std_x, std_y = train_x.std(), train_y.std()\n",
    "\n",
    "    train_x = (train_x - mean_x) /  std_x\n",
    "    test_x = (test_x - mean_x) / std_x\n",
    "    \n",
    "    assert np.all(np.isfinite(train_x))\n",
    "    \n",
    "    train_y = (train_y - mean_y) / std_y\n",
    "    test_y = (test_y - mean_y) / std_y\n",
    "\n",
    "    features = keep_columns.index.values[keep_columns.values]\n",
    "    return features, train_x, train_y, test_x, test_y, mean_y, std_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_denormalized_mse(std_y):\n",
    "    def denormalized_mse(y_true, y_pred):\n",
    "        # model is trained with normalized data, but we want\n",
    "        # mse on not normalized data to compare with MOST\n",
    "        mse = K.mean(K.square(y_true - y_pred), axis=-1)\n",
    "        return mse * std_y**2\n",
    "    return denormalized_mse\n",
    "\n",
    "\n",
    "def orthonormal_regularizer(regu):\n",
    "    # eq. 3 in https://arxiv.org/pdf/1703.01827.pdf\n",
    "    def compute(weight_matrix):\n",
    "        rows, cols = weight_matrix.shape\n",
    "        wtw = K.dot(K.transpose(weight_matrix), weight_matrix)\n",
    "        return regu * K.sum((wtw - K.eye(cols.value))**2) / 2\n",
    "    return compute\n",
    "\n",
    "\n",
    "def build_model(sizes, std_y=1):\n",
    "    # every element in sizes specifies a layer\n",
    "    #   negative number: skip connection of -n layers\n",
    "    #                    successive skips are aggregated\n",
    "    #                    into a single layer\n",
    "    #   0<n<1: dropout with pkeep=n\n",
    "    #   >1 fully connected then prelu\n",
    "    layers = [Input(shape=(sizes[0],))]\n",
    "    i = 1\n",
    "    while i < len(sizes):\n",
    "        num = sizes[i]\n",
    "        if num < 0:\n",
    "            skip = [layers[-1]]\n",
    "            while i < len(sizes) and sizes[i] < 0:\n",
    "                skip.append(layers[sizes[i] - 1])\n",
    "                i += 1\n",
    "            layer = Concatenate()(skip)\n",
    "            i -= 1\n",
    "        elif num < 1:\n",
    "            layer = Dropout(num)(layers[-1])\n",
    "        else:\n",
    "            layer = PReLU()(\n",
    "                BatchNormalization()(\n",
    "                    Dense(num, kernel_initializer=VarianceScaling(2, 'fan_in'),\n",
    "                          kernel_regularizer=regularizers.l2(0.0005))(\n",
    "                        layers[-1]\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "        layers.append(layer)\n",
    "        i += 1\n",
    "\n",
    "    layers.append(Dense(1)(layers[-1]))\n",
    "\n",
    "    opt = Adam(lr=0.001)\n",
    "    model = Model(inputs=layers[0], outputs=layers[-1])\n",
    "    model.compile(loss='mse', optimizer=opt, metrics=[compute_denormalized_mse(std_y)])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(network, use_trend, feature_level, batch_size=1024, verbose=2):\n",
    "    ddf, features = get_features(df, use_trend, feature_level)\n",
    "    features, train_x, train_y, test_x, test_y, mean_y, std_y = get_train_test_data(ddf, features, 'phi_m')\n",
    "\n",
    "    K.clear_session()  # https://stackoverflow.com/q/35114376/521776\n",
    "    model = build_model([len(features)] + network, std_y=std_y)\n",
    "\n",
    "    dtime = datetime.datetime.utcnow().isoformat().replace('-', '').replace(':', '').replace('T', '-')[:-7]\n",
    "    logdir = 'dev/logs/%s-tren:%s-features:%s-batchsize:%s-nparam:%s/' % (\n",
    "        dtime, use_trend, feature_level, batch_size, model.count_params()\n",
    "    )\n",
    "\n",
    "    if verbose > 0:\n",
    "        print('Saving to', logdir)\n",
    "\n",
    "    callbacks = [\n",
    "        ReduceLROnPlateau(factor=0.1, verbose=verbose, min_lr=1e-6, patience=10),\n",
    "        ModelCheckpoint(logdir + 'weights-w.{epoch:04d}-{val_loss:.4f}.hdf5',\n",
    "                        verbose=verbose, save_best_only=True),\n",
    "        TensorBoard(logdir, write_graph=True, write_grads=True, histogram_freq=0),\n",
    "        #EarlyStopping(min_delta=0.0001, patience=50),\n",
    "    ]\n",
    "\n",
    "    hist = model.fit(\n",
    "        train_x, train_y,\n",
    "        batch_size=batch_size,\n",
    "        epochs=1000,\n",
    "        verbose=verbose,\n",
    "        shuffle=True,\n",
    "        callbacks=callbacks,\n",
    "        validation_data=(test_x, test_y)\n",
    "    )\n",
    "\n",
    "    return hist, logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/ipykernel_launcher.py:43: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to dev/logs/20180326-135907-tren:True-features:4-batchsize:1024-nparam:193510/\n",
      "Train on 1180167 samples, validate on 83654 samples\n",
      "Epoch 1/1000\n",
      "1180167/1180167 [==============================] - 60s 50us/step - loss: 0.8064 - denormalized_mse: 0.5618 - val_loss: 0.4727 - val_denormalized_mse: 0.4785\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.47267, saving model to dev/logs/20180326-135907-tren:True-features:4-batchsize:1024-nparam:193510/weights-w.0001-0.4727.hdf5\n",
      "Epoch 2/1000\n",
      "1180167/1180167 [==============================] - 60s 51us/step - loss: 0.2788 - denormalized_mse: 0.3239 - val_loss: 0.2633 - val_denormalized_mse: 0.4469\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.47267 to 0.26327, saving model to dev/logs/20180326-135907-tren:True-features:4-batchsize:1024-nparam:193510/weights-w.0002-0.2633.hdf5\n",
      "Epoch 3/1000\n",
      "1180167/1180167 [==============================] - 60s 51us/step - loss: 0.1844 - denormalized_mse: 0.3166 - val_loss: 0.1948 - val_denormalized_mse: 0.3669\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.26327 to 0.19478, saving model to dev/logs/20180326-135907-tren:True-features:4-batchsize:1024-nparam:193510/weights-w.0003-0.1948.hdf5\n",
      "Epoch 4/1000\n",
      "1180167/1180167 [==============================] - 57s 48us/step - loss: 0.1652 - denormalized_mse: 0.3113 - val_loss: 0.1697 - val_denormalized_mse: 0.3294\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.19478 to 0.16968, saving model to dev/logs/20180326-135907-tren:True-features:4-batchsize:1024-nparam:193510/weights-w.0004-0.1697.hdf5\n",
      "Epoch 5/1000\n",
      "1180167/1180167 [==============================] - 57s 48us/step - loss: 0.1580 - denormalized_mse: 0.3082 - val_loss: 0.1831 - val_denormalized_mse: 0.3668\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/1000\n",
      "1180167/1180167 [==============================] - 55s 47us/step - loss: 0.1551 - denormalized_mse: 0.3048 - val_loss: 0.3437 - val_denormalized_mse: 0.7257\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/1000\n",
      "1180167/1180167 [==============================] - 56s 47us/step - loss: 0.1507 - denormalized_mse: 0.3006 - val_loss: 0.1618 - val_denormalized_mse: 0.3293\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.16968 to 0.16181, saving model to dev/logs/20180326-135907-tren:True-features:4-batchsize:1024-nparam:193510/weights-w.0007-0.1618.hdf5\n",
      "Epoch 8/1000\n",
      "1180167/1180167 [==============================] - 57s 48us/step - loss: 0.1470 - denormalized_mse: 0.2973 - val_loss: 0.2164 - val_denormalized_mse: 0.4540\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/1000\n",
      "1180167/1180167 [==============================] - 55s 47us/step - loss: 0.1452 - denormalized_mse: 0.2957 - val_loss: 0.2120 - val_denormalized_mse: 0.4473\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/1000\n",
      "1180167/1180167 [==============================] - 56s 47us/step - loss: 0.1428 - denormalized_mse: 0.2929 - val_loss: 0.1467 - val_denormalized_mse: 0.3027\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.16181 to 0.14670, saving model to dev/logs/20180326-135907-tren:True-features:4-batchsize:1024-nparam:193510/weights-w.0010-0.1467.hdf5\n",
      "Epoch 11/1000\n",
      "1180167/1180167 [==============================] - 61s 52us/step - loss: 0.1414 - denormalized_mse: 0.2919 - val_loss: 0.1728 - val_denormalized_mse: 0.3625\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/1000\n",
      "1180167/1180167 [==============================] - 57s 48us/step - loss: 0.1418 - denormalized_mse: 0.2917 - val_loss: 0.5793 - val_denormalized_mse: 1.2711\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/1000\n",
      "1180167/1180167 [==============================] - 56s 47us/step - loss: 0.1394 - denormalized_mse: 0.2887 - val_loss: 0.1519 - val_denormalized_mse: 0.3177\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/1000\n",
      "1180167/1180167 [==============================] - 56s 47us/step - loss: 0.1385 - denormalized_mse: 0.2879 - val_loss: 0.2350 - val_denormalized_mse: 0.5047\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/1000\n",
      "1180167/1180167 [==============================] - 56s 48us/step - loss: 0.1373 - denormalized_mse: 0.2864 - val_loss: 0.1437 - val_denormalized_mse: 0.3010\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.14670 to 0.14371, saving model to dev/logs/20180326-135907-tren:True-features:4-batchsize:1024-nparam:193510/weights-w.0015-0.1437.hdf5\n",
      "Epoch 16/1000\n",
      "1180167/1180167 [==============================] - 57s 49us/step - loss: 0.1382 - denormalized_mse: 0.2880 - val_loss: 0.1929 - val_denormalized_mse: 0.4100\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/1000\n",
      "1180167/1180167 [==============================] - 56s 48us/step - loss: 0.1360 - denormalized_mse: 0.2842 - val_loss: 0.2360 - val_denormalized_mse: 0.5084\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/1000\n",
      "1180167/1180167 [==============================] - 57s 49us/step - loss: 0.1357 - denormalized_mse: 0.2841 - val_loss: 0.2771 - val_denormalized_mse: 0.6006\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/1000\n",
      "1180167/1180167 [==============================] - 59s 50us/step - loss: 0.1352 - denormalized_mse: 0.2835 - val_loss: 0.2115 - val_denormalized_mse: 0.4530\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/1000\n",
      "1180167/1180167 [==============================] - 58s 49us/step - loss: 0.1350 - denormalized_mse: 0.2831 - val_loss: 0.3505 - val_denormalized_mse: 0.7653\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 21/1000\n",
      "1180167/1180167 [==============================] - 56s 48us/step - loss: 0.1341 - denormalized_mse: 0.2816 - val_loss: 0.2901 - val_denormalized_mse: 0.6306\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/1000\n",
      "1143808/1180167 [============================>.] - ETA: 1s - loss: 0.1347 - denormalized_mse: 0.2830"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-196f092e6514>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mfeature_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-37-e85995d6f351>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(network, use_trend, feature_level, batch_size, verbose)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     )\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hist, logdir = run_experiment(\n",
    "    [256, 256, 256, 128, 64, 64, 32, 32, 16, 8, 4, 2, 1],\n",
    "    use_trend=True,\n",
    "    feature_level=4,\n",
    "    batch_size=1024,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/ipykernel_launcher.py:43: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bsize 1024 fset 1 trend True network 0 best 0.339911 @ 84 - logdir dev/logs/20180324-194659-tren:True-features:1-batchsize:1024-nparam:53802/\n",
      "bsize 1024 fset 1 trend True network 1 best 0.307802 @ 62 - logdir dev/logs/20180324-202848-tren:True-features:1-batchsize:1024-nparam:54890/\n",
      "bsize 1024 fset 2 trend True network 0 best 0.302378 @ 69 - logdir dev/logs/20180324-210314-tren:True-features:2-batchsize:1024-nparam:54314/\n",
      "bsize 1024 fset 2 trend True network 1 best 0.304889 @ 71 - logdir dev/logs/20180324-213826-tren:True-features:2-batchsize:1024-nparam:55402/\n",
      "bsize 1024 fset 3 trend True network 0 best 0.292065 @ 63 - logdir dev/logs/20180324-221607-tren:True-features:3-batchsize:1024-nparam:54826/\n",
      "bsize 1024 fset 3 trend True network 1 best 0.341736 @ 29 - logdir dev/logs/20180324-224938-tren:True-features:3-batchsize:1024-nparam:55914/\n",
      "bsize 1024 fset 4 trend True network 0 best 0.306433 @ 59 - logdir dev/logs/20180324-231314-tren:True-features:4-batchsize:1024-nparam:55850/\n",
      "bsize 1024 fset 4 trend True network 1 best 0.244598 @ 76 - logdir dev/logs/20180324-234608-tren:True-features:4-batchsize:1024-nparam:56938/\n",
      "bsize 1024 fset 5 trend True network 0 best 0.277206 @ 83 - logdir dev/logs/20180325-002603-tren:True-features:5-batchsize:1024-nparam:56874/\n",
      "bsize 1024 fset 5 trend True network 1 best 0.252249 @ 65 - logdir dev/logs/20180325-010642-tren:True-features:5-batchsize:1024-nparam:57962/\n",
      "bsize 1024 fset 1 trend False network 0 best 0.391637 @ 31 - logdir dev/logs/20180325-014307-tren:False-features:1-batchsize:1024-nparam:51498/\n",
      "bsize 1024 fset 1 trend False network 1 best 0.346011 @ 23 - logdir dev/logs/20180325-020658-tren:False-features:1-batchsize:1024-nparam:52586/\n",
      "bsize 1024 fset 2 trend False network 0 best 0.324900 @ 11 - logdir dev/logs/20180325-022943-tren:False-features:2-batchsize:1024-nparam:51754/\n",
      "bsize 1024 fset 2 trend False network 1 best 0.289282 @ 18 - logdir dev/logs/20180325-024759-tren:False-features:2-batchsize:1024-nparam:52842/\n",
      "bsize 1024 fset 3 trend False network 0 best 0.320737 @ 20 - logdir dev/logs/20180325-030910-tren:False-features:3-batchsize:1024-nparam:52010/\n",
      "bsize 1024 fset 3 trend False network 1 best 0.313763 @ 33 - logdir dev/logs/20180325-033006-tren:False-features:3-batchsize:1024-nparam:53098/\n",
      "bsize 1024 fset 4 trend False network 0 best 0.323527 @ 18 - logdir dev/logs/20180325-035606-tren:False-features:4-batchsize:1024-nparam:52522/\n",
      "bsize 1024 fset 4 trend False network 1 best 0.285501 @ 63 - logdir dev/logs/20180325-041625-tren:False-features:4-batchsize:1024-nparam:53610/\n",
      "bsize 1024 fset 5 trend False network 0 best 0.276768 @ 69 - logdir dev/logs/20180325-045141-tren:False-features:5-batchsize:1024-nparam:53034/\n",
      "bsize 1024 fset 5 trend False network 1 best 0.238060 @ 12 - logdir dev/logs/20180325-052718-tren:False-features:5-batchsize:1024-nparam:54122/\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    #[64, 32, 16, 8, 4, 2, 1],\n",
    "    #[128, 64, 32, 16, 8, 4, 2, 1],\n",
    "    #[256, 0.5, 128, 64, 32, 16, 8, 4, 2, 1],\n",
    "    [256, 0.5, 128, 64, 64, 32, 16, 8, 4, 2, 1], \n",
    "    [256, 0.5, 128, 64, 64, 32, 32, 16, 8, 4, 2, 1], \n",
    "]\n",
    "\n",
    "\n",
    "bsize = 1024\n",
    "for trend in [True, False]:\n",
    "    for fset in [1, 2, 3, 4, 5]:\n",
    "        for i, network in enumerate(models):\n",
    "            hist, logdir = run_experiment(network, trend, fset, bsize, verbose=0)\n",
    "            best = min(hist.history['val_denormalized_mse'])\n",
    "            best_at = np.argmin(hist.history['val_denormalized_mse'])\n",
    "\n",
    "            print('bsize %s fset %s trend %s network %s best %f @ %s - logdir %s' % (\n",
    "                bsize, fset, trend, i, best, best_at, logdir\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
