{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we run some baselines to understand the level of performance to beat\n",
    "we use nested CV and grid search to get realistic performance estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>10991</td><td>application_1526283611315_0080</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hadoop30:8088/proxy/application_1526283611315_0080/\">Link</a></td><td><a target=\"_blank\" href=\"http://hadoop20:8042/node/containerlogs/container_e47_1526283611315_0080_01_000001/more_stuff__emilio_d\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "from netCDF4 import Dataset\n",
    "from collections import defaultdict, namedtuple\n",
    "import datetime\n",
    "import json\n",
    "from json import JSONEncoder\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import base64\n",
    "import pickle\n",
    "import hashlib\n",
    "import sympy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import csv\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV, KFold, RandomizedSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import fmin_cg, fmin_ncg\n",
    "from scipy import stats\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_df():\n",
    "    dframe_path = 'hdfs:///Projects/more_stuff/cabauw/processed-full-log.csv.gz'\n",
    "\n",
    "    from hops import hdfs\n",
    "    fs = hdfs.get_fs()\n",
    "\n",
    "    with fs.open_file(dframe_path) as f:\n",
    "        df = pd.read_csv(f, na_values='--', compression='gzip')\n",
    "\n",
    "    df = df[(df.ustar > 0.1) & (abs(df.H) > 10) & (df.wind > 1)]\n",
    "    df = df[df.ds != 201603]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_index(dtimes, interval):\n",
    "    # returns a tuple index_above, index_below\n",
    "    # index_above[i] is the largest i\n",
    "    # such that dtimes[index_above[i]] - dtimes[i] < interval\n",
    "    # index_below[i] is the smallest i\n",
    "    # such that dtimes[i] - dtimes[index_below[i]] < interval\n",
    "    # dtimes must be already sorted!\n",
    "    index_below, index_above = np.zeros(\n",
    "        (2, len(dtimes)), dtype=np.int\n",
    "    ) - 1\n",
    "\n",
    "    for i, x in enumerate(dtimes):\n",
    "        j = index_below[i - 1] if i > 0 else 0\n",
    "        while x - dtimes[j] > interval:\n",
    "            j += 1\n",
    "\n",
    "        index_below[i] = j\n",
    "        index_above[j] = i\n",
    "\n",
    "    last_above = index_above[0]\n",
    "    for i in range(len(dtimes)):\n",
    "        if index_above[i] < 0:\n",
    "            index_above[i] = last_above\n",
    "        else:\n",
    "            last_above = index_above[i]\n",
    "    \n",
    "    return index_above, index_below\n",
    "\n",
    "\n",
    "def compute_trend(df, columns, interval=3600):\n",
    "    df = df.sort_values('datetime')\n",
    "    for z in df.z.unique():  \n",
    "        this_level = df[df.z == z]\n",
    "        index_above, index_below = make_index(this_level.datetime.values, interval)\n",
    "\n",
    "        for col in columns:\n",
    "            val_above = this_level[col].values\n",
    "            val_below = this_level.iloc[index_below][col].values\n",
    "\n",
    "            time_above = this_level.datetime.values\n",
    "            time_below = this_level.iloc[index_below].datetime.values\n",
    "\n",
    "            trend = 3600 * (val_above - val_below) / (time_above - time_below)\n",
    "\n",
    "            df.loc[df.z == z, col + '_trend'] = trend\n",
    "\n",
    "    return df, [col + '_trend' for col in columns]\n",
    "\n",
    "\n",
    "def get_features(df, use_trend, feature_level):\n",
    "    # get feature names of the corresponding level\n",
    "    # adding them to df if not already there\n",
    "\n",
    "    feature_sets = [\n",
    "        [\n",
    "            'z', 'wind', 'temp', 'soil_temp',\n",
    "            'wind_10', 'wind_20', 'wind_40',\n",
    "            'temp_10', 'temp_20', 'temp_40',\n",
    "        ],\n",
    "        ['soilheat'],\n",
    "        ['netrad'],\n",
    "        ['rain', 'dewpoint'],\n",
    "        ['H', 'LE'],\n",
    "    ]\n",
    "\n",
    "    if isinstance(feature_level, int):\n",
    "        features = [\n",
    "            f for fset in feature_sets[:feature_level]\n",
    "            for f in fset\n",
    "        ]\n",
    "    elif isinstance(feature_level, (list, tuple)):\n",
    "        features = feature_level\n",
    "    else:\n",
    "        raise ValueError('pass list or int')\n",
    "\n",
    "    \n",
    "    if ('wind_40' not in df.columns or\n",
    "            'wind_20' not in df.columns or\n",
    "            'wind_10' not in df.columns):\n",
    "\n",
    "        wind_temp_levels = df.pivot_table(\n",
    "            values=['wind', 'temp'], columns='z', index=['ds', 'tt']\n",
    "        ).reset_index()\n",
    "        wind_temp_levels.columns = [\n",
    "            '%s_%d' % (a, b) if b else a\n",
    "            for a, b in wind_temp_levels.columns.values\n",
    "        ]\n",
    "        df = df.merge(wind_temp_levels, on=['ds', 'tt'])\n",
    "\n",
    "    if use_trend:\n",
    "        missing_trend = [\n",
    "            f for f in features\n",
    "            if f != 'z' and f + '_trend' not in df.columns\n",
    "        ]\n",
    "\n",
    "        if missing_trend:\n",
    "            df, added_cols = compute_trend(df, missing_trend)\n",
    "            features.extend(added_cols)\n",
    "\n",
    "    # remove feature columns with only nulls and rows with any null\n",
    "    empty_columns = df.isnull().all(axis=0)\n",
    "    keep_columns = df.columns.isin(features) & ~empty_columns\n",
    "    missing = df.loc[:, keep_columns].isnull().any(axis=1)\n",
    "    df = df[~missing]\n",
    "    features = keep_columns.index.values[keep_columns.values]\n",
    "\n",
    "    return df, features\n",
    "\n",
    "\n",
    "def get_train_test(df, features, target, train_idx, test_idx, normalize):\n",
    "    train_x, train_y = df.iloc[train_idx][features], df.iloc[train_idx][target]\n",
    "    test_x, test_y = df.iloc[test_idx][features], df.iloc[test_idx][target]\n",
    "\n",
    "    if normalize:\n",
    "        mean_x, std_x = train_x.mean(), train_x.std()\n",
    "        train_x = (train_x - mean_x) / std_x\n",
    "        test_x = (test_x - mean_x) / std_x\n",
    "\n",
    "        mean_y, std_y = train_y.mean(), train_y.std()\n",
    "        train_y = (train_y - mean_y) / std_y\n",
    "        test_y = (test_y - mean_y) / std_y\n",
    "    else:\n",
    "        mean_y, std_y = 0, 1\n",
    "\n",
    "    return train_x, train_y, test_x, test_y, mean_y, std_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MOSTEstimator:\n",
    "    ''' estimator for the universal functions in the monin-obukhov similarity theory\n",
    "        implementing scikit's interface\n",
    "        \n",
    "        fitting is done by minimizing the L2 regularized squared error\n",
    "        via conjugate gradient\n",
    "    '''\n",
    "    def __init__(self, regu=0.1, use_hessian=True):\n",
    "        self.regu = regu\n",
    "        self.a, self.b, self.c, self.d = (1, 4.8, np.sqrt(19.3), -0.25)\n",
    "        self.use_hessian = use_hessian\n",
    "        self.symbols = None\n",
    "\n",
    "    def _lazy_init_hessian(self):\n",
    "        # we initialize these functions lazily so that we can pickle\n",
    "        # this object and send it around before fitting\n",
    "        if self.use_hessian and self.symbols is None:\n",
    "            a, b, c, d, x = sp.symbols('a b c d x')\n",
    "            self.symbols = a, b, c, d\n",
    "\n",
    "            self._neg_H_fn = self._get_hessian_functions(\n",
    "                a * sp.Pow(1 - x * c**2, d), x, a, b, c, d\n",
    "            )\n",
    "            self._pos_H_fn = self._get_hessian_functions(\n",
    "                a + b * x, x, a, b, c, d\n",
    "            )\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_hessian_functions(expr, x, *symbols):\n",
    "        # returns functions computing second-order partial derivatives\n",
    "        # of expr. keyed by differentiation variables\n",
    "        return {\n",
    "            (s1, s2): sp.lambdify(\n",
    "                [x] + list(symbols),\n",
    "                sp.simplify(sp.diff(sp.diff(expr, s1), s2)),\n",
    "                'numpy'\n",
    "            ) for s1 in symbols for s2 in symbols\n",
    "        }\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {'regu': self.regu}\n",
    "\n",
    "    def set_params(self, regu):\n",
    "        self.regu = regu\n",
    "        return self\n",
    "\n",
    "    @classmethod\n",
    "    def _compute_phi(cls, zL, a, b, c, d):\n",
    "        zL = cls._to_vec(zL)\n",
    "        mask = zL >= 0\n",
    "        yy = np.zeros(zL.shape)\n",
    "        yy[mask] = a + b * zL[mask]\n",
    "        yy[~mask] = a * np.power(1 - c**2 * zL[~mask], d)\n",
    "        assert all(np.isfinite(zL))\n",
    "        assert all(np.isfinite(yy)), (a, b, c, d)\n",
    "        return yy\n",
    "\n",
    "    @classmethod\n",
    "    def _compute_phi_prime(cls, zL, a, b, c, d):\n",
    "        zL = cls._to_vec(zL)\n",
    "        dpda, dpdb, dpdc, dpdd = np.zeros((4, len(zL)))\n",
    "\n",
    "        pos, neg = zL >= 0, zL < 0\n",
    "\n",
    "        dpda[pos] = 1\n",
    "        dpdb[pos] = zL[pos]\n",
    "\n",
    "        inner = 1 - c**2 * zL[neg]\n",
    "        dpda[neg] = np.power(inner, d)\n",
    "        dpdc[neg] = -2 * zL[neg] * a * c * d * np.power(inner, d - 1)\n",
    "        dpdd[neg] = a * np.log(inner) * np.power(inner, d)\n",
    "\n",
    "        return dpda, dpdb, dpdc, dpdd\n",
    "\n",
    "    def _fmin_hess(self, params, xx, yy, regu):\n",
    "        self._lazy_init_hessian()\n",
    "\n",
    "        preds = self._compute_phi(xx, *params)\n",
    "        xpos_mask = xx >= 0\n",
    "        hh1 = np.zeros((4, 4))\n",
    "\n",
    "        for i, s1 in enumerate(self.symbols):\n",
    "            for j, s2 in enumerate(self.symbols):\n",
    "                # when xx >= 0 the function is linear\n",
    "                # its hessian is always 0\n",
    "\n",
    "                neg = self._neg_H_fn[s1, s2](xx[~xpos_mask], *params)\n",
    "                hh1[i, j] = np.sum((preds[~xpos_mask] - yy[~xpos_mask]) * neg)\n",
    "\n",
    "        hh2 = np.zeros((4, len(xx)))\n",
    "        hh2[:, :] = self._compute_phi_prime(xx, *params)\n",
    "        \n",
    "        hess = 2 * ((hh2.dot(hh2.T) + hh1) / len(xx) + regu * np.eye(4))\n",
    "        return hess\n",
    "\n",
    "    @staticmethod\n",
    "    def _fmin_target(params, xx, yy, regu):\n",
    "        preds = MOSTEstimator._compute_phi(xx, *params)\n",
    "        err = np.mean((yy - preds)**2) + regu * sum(p**2 for p in params)\n",
    "        return err\n",
    "\n",
    "    @staticmethod\n",
    "    def _fmin_grad(params, xx, yy, regu):\n",
    "        preds = MOSTEstimator._compute_phi(xx, *params)\n",
    "        der = MOSTEstimator._compute_phi_prime(xx, *params)\n",
    "\n",
    "        grads = [\n",
    "            2 * np.mean((preds - yy) * parpr) + 2 * regu * par\n",
    "            for par, parpr in zip(params, der)\n",
    "        ]\n",
    "\n",
    "        return np.array(grads)\n",
    "\n",
    "    @staticmethod\n",
    "    def _to_vec(mat):\n",
    "        mat = np.array(mat)\n",
    "        \n",
    "        # check that multi-dimensional arrays have only one\n",
    "        # dimension with more than one sample\n",
    "        # e.g. 1x1x99x1 is fine, 1x2x99x is not\n",
    "        assert sum(1 for n in mat.shape if n > 1) == 1\n",
    "        return mat.reshape(-1)\n",
    "    \n",
    "    def fit(self, X, y, disp=False):\n",
    "        X = self._to_vec(X)\n",
    "        y = self._to_vec(y)\n",
    "\n",
    "        if self.use_hessian:\n",
    "            self.a, self.b, self.c, self.d = fmin_ncg(\n",
    "                self._fmin_target,\n",
    "                (self.a, self.b, self.c, self.d),\n",
    "                self._fmin_grad,\n",
    "                fhess=self._fmin_hess,\n",
    "                args=(X, y, self.regu),\n",
    "                disp=disp,\n",
    "            )\n",
    "        else:\n",
    "            self.a, self.b, self.c, self.d = fmin_cg(\n",
    "                self._fmin_target,\n",
    "                (self.a, self.b, self.c, self.d),\n",
    "                self._fmin_grad,\n",
    "                args=(X, y, self.regu),\n",
    "                disp=disp,\n",
    "            )\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self._compute_phi(X, self.a, self.b, self.c, self.d)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        preds = self.predict(X)\n",
    "        return metrics.mean_squared_error(y, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttributeKFold:\n",
    "    ''' k-fold cross validator splitting on a particular attribute\n",
    "        so that all samples with a given value are either in the train or test set\n",
    "\n",
    "        attribute value for each sample is given in the constructor, so that\n",
    "        the attribute itself need not be in the features for the model\n",
    "    '''\n",
    "    def __init__(self, cv, attr):\n",
    "        self.cv, self.attr = cv, attr\n",
    "\n",
    "    def get_n_splits(self, *args, **kwargs):\n",
    "        return self.cv.get_n_splits(*args, **kwargs)\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        vals = self.attr.unique()\n",
    "        for train_idx, test_idx in self.cv.split(vals):\n",
    "            train_mask = self.attr.isin(vals[train_idx])\n",
    "            test_mask = self.attr.isin(vals[test_idx])\n",
    "\n",
    "            yield (\n",
    "                np.argwhere(train_mask).reshape(-1),\n",
    "                np.argwhere(test_mask).reshape(-1),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_attributekfold():\n",
    "    outer_cv = AttributeKFold(KFold(10, shuffle=True), df.ds)\n",
    "    outer_train, outer_test = np.zeros((2, len(df)))\n",
    "    for outer_train_idx, outer_test_idx in outer_cv.split(df):\n",
    "\n",
    "        outer_train[outer_train_idx] += 1\n",
    "        outer_test[outer_test_idx] += 1\n",
    "\n",
    "        inner_train, inner_test = np.zeros((2, len(outer_train_idx)))\n",
    "        inner_cv = AttributeKFold(KFold(5, shuffle=True), df.iloc[outer_train_idx].ds)\n",
    "        for inner_train_idx, inner_test_idx in inner_cv.split(df.iloc[outer_train_idx]):\n",
    "            inner_train[inner_train_idx] += 1\n",
    "            inner_test[inner_test_idx] += 1\n",
    "\n",
    "        assert all(inner_train == 4)\n",
    "        assert all(inner_test == 1)\n",
    "\n",
    "    assert all(outer_train == 9)\n",
    "    assert all(outer_test == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_preds(ypred, ytrue):\n",
    "    minn = max(min(ypred), min(ytrue))\n",
    "    maxx = min(max(ypred), max(ytrue))\n",
    "    \n",
    "    plt.scatter(ytrue, ypred, s=2)\n",
    "    plt.plot([minn, maxx], [minn, maxx], 'r--')\n",
    "    plt.xlabel('True')\n",
    "    plt.ylabel('Predicted')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogUniform:\n",
    "    ''' random variable X such that log(x) is distributed uniformly\n",
    "    '''\n",
    "    def __init__(self, base, expmin, expmax):\n",
    "        self.base, self.expmin, self.expmax = base, expmin, expmax\n",
    "\n",
    "    def rvs(self, size=None, random_state=None):\n",
    "        random_state = random_state or np.random.RandomState()\n",
    "        exp = random_state.uniform(self.expmin, self.expmax, size=size)\n",
    "        return np.power(self.base, exp)\n",
    "\n",
    "\n",
    "class IntDistribution:\n",
    "    ''' random variable taking only integer values\n",
    "    '''\n",
    "    def __init__(self, rv):\n",
    "        self.rv = rv\n",
    "\n",
    "    def rvs(self, *args, **kwargs):\n",
    "        sample = self.rv.rvs(*args, **kwargs)\n",
    "        return int(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CVSpec = namedtuple('CVSpec', [\n",
    "    'model', 'param_distribution', 'features',\n",
    "    'target', 'inner_cv', 'outer_cv', 'n_iter', 'normalize',\n",
    "    'inner_seed', 'outer_seed', 'param_seed', 'meta',\n",
    "    'save_to',\n",
    "])\n",
    "\n",
    "CVResult = namedtuple('CVResult', [\n",
    "    'meta', 'scores', 'test_x', 'test_y', 'y_pred', 'imps'\n",
    "])\n",
    "\n",
    "\n",
    "# set default value to None, https://stackoverflow.com/a/18348004/521776\n",
    "CVSpec.__new__.__defaults__ = (None,) * len(CVSpec._fields)\n",
    "CVResult.__new__.__defaults__ = (None,) * len(CVResult._fields)\n",
    "\n",
    "Scores = namedtuple('Scores', [\n",
    "    'train_mse',\n",
    "    'explained_variance_score',\n",
    "    'mean_absolute_error',\n",
    "    'mean_squared_error',\n",
    "    'median_absolute_error',\n",
    "    'r2_score',\n",
    "    'mean_abs_percent_error',\n",
    "    'median_abs_percent_error',\n",
    "])\n",
    "\n",
    "OuterCVResult = namedtuple('OuterCVResult', [\n",
    "    'scores',\n",
    "    'param_keys',\n",
    "    'param_values',\n",
    "    'test_x',\n",
    "    'test_y',\n",
    "    'y_pred',\n",
    "])\n",
    "\n",
    "InnerCVResult = namedtuple('InnerCVResult', [\n",
    "    'inner_mse', 'final_results'\n",
    "])\n",
    "\n",
    "\n",
    "def get_cv_fold(fold, cv_k, seed, attr):\n",
    "    assert fold < cv_k\n",
    "    \n",
    "    cv = AttributeKFold(\n",
    "        KFold(cv_k, shuffle=True, random_state=seed),\n",
    "        attr\n",
    "    ).split(attr)\n",
    "    \n",
    "    for _ in range(fold):\n",
    "        _ = next(cv)\n",
    "    return next(cv)\n",
    "\n",
    "\n",
    "def get_train_test(df, features, target, train_idx, test_idx, normalize):\n",
    "    train_x, train_y = df.iloc[train_idx][features], df.iloc[train_idx][target]\n",
    "    test_x, test_y = df.iloc[test_idx][features], df.iloc[test_idx][target]\n",
    "\n",
    "    if normalize:\n",
    "        mean_x, std_x = train_x.mean(), train_x.std()\n",
    "        train_x = (train_x - mean_x) / std_x\n",
    "        test_x = (test_x - mean_x) / std_x\n",
    "\n",
    "        mean_y, std_y = train_y.mean(), train_y.std()\n",
    "        train_y = (train_y - mean_y) / std_y\n",
    "        test_y = (test_y - mean_y) / std_y\n",
    "    else:\n",
    "        mean_y, std_y = 0, 1\n",
    "\n",
    "    return train_x, train_y, test_x, test_y, mean_y, std_y\n",
    "\n",
    "\n",
    "class EncodeMoreStuff(JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return list(obj)\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "\n",
    "class CachedResults:\n",
    "    def __init__(self, **kwargs):\n",
    "        try:\n",
    "            from hops import hdfs\n",
    "        except ImportError:\n",
    "            self.open_ = lambda f, m: open(f, m + 'b')\n",
    "            self.cache_dir = './dev/checkpoints/'\n",
    "        else:\n",
    "            fs = hdfs.get_fs()\n",
    "            self.open_ = lambda f, m: fs.open_file(f, m)\n",
    "            \n",
    "            # using HDFS as a filesystem cache\n",
    "            # is a *terrible* idea in so many ways\n",
    "            # but we have no alternative.\n",
    "            self.cache_dir = 'hdfs:///Projects/more_stuff/cabauw/checkpoints/'\n",
    "\n",
    "        self.fname = hashlib.sha256(\n",
    "            json.dumps(kwargs, cls=EncodeMoreStuff).encode('utf8')\n",
    "        ).hexdigest()\n",
    "    \n",
    "    def get_value(self):\n",
    "        return\n",
    "        try:\n",
    "            with self.open_(self.cache_dir + self.fname, 'r') as f:\n",
    "                return pickle.loads(f.read())\n",
    "        except:  # want to invalidate cache regardless of what went wrong\n",
    "            return None\n",
    "\n",
    "    def set_value(self, val):\n",
    "        return\n",
    "        with self.open_(self.cache_dir + self.fname, 'w') as f:\n",
    "            f.write(pickle.dumps(val))\n",
    "\n",
    "\n",
    "def compute_metrics(train_y, y_pred_train, test_y, y_pred, mean_y, std_y):\n",
    "    y_pred = y_pred * std_y + mean_y\n",
    "    y_pred_train = y_pred_train * std_y + mean_y\n",
    "\n",
    "    test_y = test_y * std_y + mean_y\n",
    "    train_y = train_y * std_y + mean_y\n",
    "\n",
    "    perc_errors = 100 * np.abs((test_y - y_pred) / test_y)\n",
    "\n",
    "    return Scores(\n",
    "        train_mse=metrics.mean_squared_error(train_y, y_pred_train),\n",
    "        explained_variance_score=metrics.explained_variance_score(test_y, y_pred),\n",
    "        mean_absolute_error=metrics.mean_absolute_error(test_y, y_pred),\n",
    "        mean_squared_error=metrics.mean_squared_error(test_y, y_pred),\n",
    "        median_absolute_error=metrics.median_absolute_error(test_y, y_pred),\n",
    "        r2_score=metrics.r2_score(test_y, y_pred),\n",
    "        mean_abs_percent_error=np.mean(perc_errors),\n",
    "        median_abs_percent_error=np.median(perc_errors),\n",
    "    )\n",
    "\n",
    "\n",
    "def inner_train(df_bcast, model, features, target, params, outer_cv,\n",
    "                outer_fold, inner_cv, inner_fold, keys, outer_seed,\n",
    "                inner_seed, normalize):\n",
    "    # pass outer_cv <= 0 to have plain CV instead of nested CV\n",
    "\n",
    "    cache = CachedResults(\n",
    "        model=model.__class__.__name__, features=features, target=target,\n",
    "        params=params, outer_fold=outer_fold, inner_fold=inner_fold, keys=keys,\n",
    "        outer_seed=outer_seed, inner_seed=inner_seed, normalize=normalize\n",
    "    )\n",
    "    saved = cache.get_value()\n",
    "    if saved is not None:\n",
    "        return saved\n",
    "\n",
    "    df = df_bcast.value\n",
    "    if any(f not in df.columns for f in features):\n",
    "        print('some features are missing, reloading...')\n",
    "        df, _ = get_features(read_df(), use_trend=True, feature_level=5)\n",
    "    assert all(f in df.columns for f in features), (df.columns, features)\n",
    "\n",
    "    if outer_cv > 0:\n",
    "        outer_train_idx, outer_test_idx = get_cv_fold(\n",
    "            outer_fold, outer_cv, outer_seed, df.ds\n",
    "        )\n",
    "\n",
    "        inner_train_idx, inner_test_idx = get_cv_fold(\n",
    "            inner_fold, inner_cv, inner_seed, df.iloc[outer_train_idx].ds\n",
    "        )\n",
    "\n",
    "        train_idx = outer_train_idx[inner_train_idx]\n",
    "        test_idx = outer_train_idx[inner_test_idx]\n",
    "    else:\n",
    "        train_idx, test_idx = get_cv_fold(\n",
    "            inner_fold, inner_cv, inner_seed, df.ds\n",
    "        )\n",
    "\n",
    "    train_x, train_y, test_x, test_y, mean_y, std_y = get_train_test(\n",
    "        df, features, target, train_idx, test_idx, normalize\n",
    "    )\n",
    "\n",
    "    model = model.set_params(**dict(zip(keys, params)))\n",
    "    model.fit(train_x, train_y)\n",
    "    y_pred_train = model.predict(train_x)\n",
    "    y_pred = model.predict(test_x)\n",
    "    \n",
    "    test_mse = metrics.mean_squared_error(test_y, y_pred)\n",
    "    \n",
    "    if outer_cv > 0:\n",
    "        final_results = None\n",
    "    else:\n",
    "        final_results = OuterCVResult(\n",
    "            scores=compute_metrics(\n",
    "                train_y, y_pred_train, test_y, y_pred, mean_y, std_y\n",
    "            ),\n",
    "            param_keys=keys,\n",
    "            param_values=params,\n",
    "            test_x=test_x if inner_fold == 0 else None,\n",
    "            test_y=test_x if inner_fold == 0 else None,\n",
    "            y_pred=test_x if inner_fold == 0 else None,\n",
    "        )\n",
    "\n",
    "    result = InnerCVResult(\n",
    "        inner_mse=test_mse,\n",
    "        final_results=final_results,\n",
    "    )\n",
    "\n",
    "    cache.set_value(result)\n",
    "    return result\n",
    "\n",
    "\n",
    "def outer_train(df_bcast, model, features, target, outer_cv, outer_fold,\n",
    "                params, keys, outer_seed, normalize):\n",
    "    cache = CachedResults(\n",
    "        model=model.__class__.__name__, features=features,\n",
    "        target=target, params=params, outer_fold=outer_fold,\n",
    "        keys=keys, outer_seed=outer_seed, normalize=normalize\n",
    "    )\n",
    "    saved = cache.get_value()\n",
    "    if saved is not None:\n",
    "        return saved\n",
    "\n",
    "    df = df_bcast.value\n",
    "    if any(f not in df.columns for f in features):\n",
    "        print('some features are missing, reloading...')\n",
    "        df, _ = get_features(read_df(), use_trend=True, feature_level=5)\n",
    "    assert all(f in df.columns for f in features), (df.columns, features)\n",
    "    \n",
    "    assert outer_cv > 0\n",
    "    train_idx, test_idx = get_cv_fold(\n",
    "        outer_fold, outer_cv, outer_seed, df.ds\n",
    "    )\n",
    "\n",
    "    train_x, train_y, test_x, test_y, mean_y, std_y = get_train_test(\n",
    "        df, features, target, train_idx, test_idx, normalize\n",
    "    )\n",
    "\n",
    "    model = model.set_params(**dict(zip(keys, params)))\n",
    "    model.fit(train_x, train_y)\n",
    "\n",
    "    y_pred_train = model.predict(train_x)\n",
    "    y_pred = model.predict(test_x)\n",
    "\n",
    "    result = OuterCVResult(\n",
    "        scores=compute_metrics(\n",
    "            train_y, y_pred_train, test_y, y_pred, mean_y, std_y\n",
    "        ),\n",
    "        param_keys=keys,\n",
    "        param_values=values,\n",
    "        test_x=test_x if inner_fold == 0 else None,\n",
    "        test_y=test_x if inner_fold == 0 else None,\n",
    "        y_pred=test_x if inner_fold == 0 else None,\n",
    "    )\n",
    "\n",
    "    cache.set_value(result)\n",
    "    return result\n",
    "\n",
    "\n",
    "def finalize_result(meta, results, save_to=None):\n",
    "    scores, params = [], []\n",
    "    test_x = test_y = y_pred = None\n",
    "    for res in list(results):\n",
    "        scores.append(res.scores)\n",
    "        params.append(dict(zip(res.param_keys, res.param_values)))\n",
    "      \n",
    "        #if res.test_x is not None:\n",
    "        #    test_x = res.test_x\n",
    "        #    \n",
    "        #if res.test_y is not None:\n",
    "        #    test_y = res.test_y\n",
    "        #    \n",
    "        #if res.y_pred is not None:\n",
    "        #    y_pred = res.y_pred\n",
    "    \n",
    "    scores_df = pd.DataFrame(scores)\n",
    "    params_df = pd.DataFrame(params)\n",
    "\n",
    "    cvres = CVResult(\n",
    "        meta=meta, scores=scores,\n",
    "        test_x=test_x, test_y=test_y, y_pred=y_pred, imps=params_df\n",
    "    )\n",
    "\n",
    "    if save_to:\n",
    "        try:\n",
    "            from hops import hdfs\n",
    "            fs = hdfs.get_fs()\n",
    "            open_ = fs.open_file\n",
    "            base_dir = 'hdfs:///Projects/more_stuff/cabauw/results/'\n",
    "        except ImportError:\n",
    "            open_ = open\n",
    "            base_dir = './data/'\n",
    "\n",
    "        pd.set_option('display.max_columns', None)\n",
    "        pd.set_option('display.max_rows', None)\n",
    "        pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "        fname = '%s/results_%s.txt' % (base_dir, save_to)\n",
    "        with open_(fname, 'w') as f:\n",
    "            f.write(scores_df.describe().T.to_string())\n",
    "            f.write('\\n\\n**raw scores\\n\\n')\n",
    "            f.write(scores_df.to_string())\n",
    "            f.write('\\n\\n**parameters\\n\\n')\n",
    "            f.write(params_df.describe().T.to_string())\n",
    "            f.write('\\n\\n**raw parameters\\n\\n')\n",
    "            f.write(params_df.to_string())\n",
    "            f.write('\\n\\n**raw json results\\n\\n')\n",
    "\n",
    "            data = base64.b64encode(pickle.dumps(cvres)).decode('utf8')\n",
    "            bufsize = 2**16\n",
    "            for buf in range(0, len(data), bufsize):\n",
    "                f.write(data[buf:buf+bufsize])\n",
    "\n",
    "    return cvres\n",
    "\n",
    "\n",
    "def make_nested_cv_tasks(df_bcast, spec, cv_vals):\n",
    "    return (sc.parallelize(cv_vals, len(cv_vals))\n",
    "\n",
    "         # train and evaluate on inner fold for each outer fold\n",
    "         # x is (outer, inner, param values)\n",
    "         # key by (outer fold, param values)\n",
    "         .map(lambda x: ((x[0], x[2]), inner_train(\n",
    "             df_bcast=df_bcast,\n",
    "             model=spec.model,\n",
    "             features=spec.features,\n",
    "             target=spec.target,\n",
    "             outer_cv=spec.outer_cv,\n",
    "             outer_fold=x[0],\n",
    "             inner_cv=spec.inner_cv,\n",
    "             inner_fold=x[1],\n",
    "             params=x[2],\n",
    "             keys=tuple(spec.param_distribution.keys()),\n",
    "             outer_seed=spec.outer_seed,\n",
    "             inner_seed=spec.inner_seed,\n",
    "             normalize=spec.normalize\n",
    "         )))\n",
    "\n",
    "         # for each outer fold/parameters, compute sum of mse\n",
    "         .reduceByKey(lambda res1, res2: InnerCVResult(\n",
    "                 inner_mse=res1.inner_mse + res2.inner_mse\n",
    "             ), numPartitions=spec.outer_cv)\n",
    "\n",
    "        # for each outer fold, find parameters with best mse\n",
    "        .map(lambda x: (x[0][0], (x[0][1], x[1])))\n",
    "        .reduceByKey(lambda x, y: x if x[1].inner_mse < y[1].inner_mse else y)\n",
    "\n",
    "        # for each outer fold, validate using best parameters\n",
    "        # x is (outer fold, (parameters, mse))\n",
    "        .map(lambda x: outer_train(\n",
    "            df_bcast=df_bcast,\n",
    "             model=spec.model,\n",
    "             features=spec.features,\n",
    "             target=spec.target,\n",
    "             outer_cv=spec.outer_cv,\n",
    "             outer_fold=x[0],\n",
    "             params=x[1][0],\n",
    "             keys=tuple(spec.param_distribution.keys()),\n",
    "             outer_seed=spec.outer_seed,\n",
    "             normalize=spec.normalize\n",
    "         ))\n",
    "\n",
    "         # finalize results, optionally saving\n",
    "         #\n",
    "         # we could use a coalesce(1) -> mapPartitions, but this would\n",
    "         # compute all the outer folds of all specs in the last stage\n",
    "         # (the one that contains collect), which means that it will only\n",
    "         # use len(cv_specs) executors to run sum(s.outer_cv for s in cv_specs)\n",
    "         # tasks. needlessly to say, that sucks\n",
    "         #\n",
    "         # by adding a keyBy/groupByKey/map, we force the previos outer cv\n",
    "         # to be in its own stage, thus there will be one partition for each\n",
    "         # outer fold of each spec, and we can fully utilize the executors\n",
    "         .keyBy(lambda x: 1)\n",
    "\n",
    "         # create one partition for each CV result\n",
    "         .groupByKey(spec.outer_cv)\n",
    "         .map(lambda x: finalize_result(spec.meta, x[1], spec.save_to))\n",
    "\n",
    "         # with this, we force finalize_result to be in its own stage\n",
    "         # \n",
    "         # since the default scheduler is FIFO, this means we will finalize\n",
    "         # the results as soon as the outer cv finishes\n",
    "         # without this, we would need to wait for all the outer cvs to finish\n",
    "         # and the results will all be finalized together at the end\n",
    "         # that would also suck\n",
    "         .coalesce(1)\n",
    "         .repartition(2))\n",
    "\n",
    "\n",
    "def make_cv_tasks(df_bcast, spec, cv_vals):\n",
    "\n",
    "    def merge_cv_results(res1, res2):\n",
    "        fin1, fin2 = res1.final_results, res2.final_results\n",
    "        \n",
    "        # make a list containing all the final results\n",
    "        #\n",
    "        # we cannot compare types directly because pyspark serialization\n",
    "        # will use a different type (that has the same name and attributes)\n",
    "        #\n",
    "        # moreover, lists are serialized as tuples, thus we need to\n",
    "        # convertthem  back to lists before joining\n",
    "        list_fin_1 = [fin1] if hasattr(fin1, 'scores') else list(fin1)\n",
    "        list_fin_2 = [fin2] if hasattr(fin2, 'scores') else list(fin2)\n",
    "        all_fin = list_fin_1 + list_fin_2\n",
    "\n",
    "        return InnerCVResult(\n",
    "            inner_mse=res1.inner_mse + res2.inner_mse,\n",
    "            final_results=all_fin,\n",
    "        )\n",
    "\n",
    "    return (sc.parallelize(cv_vals, len(cv_vals))\n",
    "        # train and evaluate on each fold\n",
    "        # x is (outer, inner, param values)\n",
    "        # key by param values\n",
    "        .map(lambda x: (x[2], inner_train(\n",
    "            df_bcast=df_bcast,\n",
    "            model=spec.model,\n",
    "            features=spec.features,\n",
    "            target=spec.target,\n",
    "            outer_cv=spec.outer_cv,\n",
    "            outer_fold=x[0],\n",
    "            inner_cv=spec.inner_cv,\n",
    "            inner_fold=x[1],\n",
    "            params=x[2],\n",
    "            keys=tuple(spec.param_distribution.keys()),\n",
    "            outer_seed=spec.outer_seed,\n",
    "            inner_seed=spec.inner_seed,\n",
    "            normalize=spec.normalize\n",
    "        )))\n",
    "\n",
    "        # for each parameters, compute sum of scores\n",
    "        # and preserve results of each fold\n",
    "        .reduceByKey(lambda res1, res2: merge_cv_results(res1, res2))\n",
    "\n",
    "        # find parameters with best mse\n",
    "        .map(lambda x: (1, x[1]))\n",
    "        .reduceByKey(lambda x, y: x if x.inner_mse < y.inner_mse else y)\n",
    "\n",
    "        # at this point we only have the InnerCVResult of the best parameters\n",
    "        # which contains the scores on each fold\n",
    "        .flatMap(lambda x: x[1].final_results)\n",
    "\n",
    "        # finalize results, optionally saving\n",
    "        .keyBy(lambda x: 1)\n",
    "        .groupByKey(spec.inner_cv)\n",
    "        .map(lambda x: finalize_result(spec.meta, x[1], spec.save_to))\n",
    "        .coalesce(1)\n",
    "        .repartition(2))\n",
    "\n",
    "\n",
    "def spark_cv(df, *cv_specs):\n",
    "    ''' cross-validation performing random search\n",
    "\n",
    "        optimized for running several nested cv in parallel, each with\n",
    "        different models and/or grids\n",
    "        \n",
    "        use outer_cv > 0 for nested cross validation\n",
    "    '''\n",
    "\n",
    "    df_bcast = sc.broadcast(df)\n",
    "    result_rdd = None\n",
    "    for spec in cv_specs:\n",
    "        # default values\n",
    "        spec = spec._replace(\n",
    "            inner_cv = spec.inner_cv or 10,\n",
    "            outer_cv = 10 if spec.outer_cv is None else spec.outer_cv,\n",
    "            n_iter = spec.n_iter or 10,\n",
    "            normalize = spec.normalize or True,\n",
    "            inner_seed = spec.inner_seed or np.random.randint(2**32, dtype=np.uint),\n",
    "            outer_seed = spec.outer_seed or np.random.randint(2**32, dtype=np.uint),   \n",
    "        )\n",
    "        \n",
    "        # build list of all grids to try and inner/outer combinations\n",
    "        rnd = np.random.RandomState(spec.param_seed)\n",
    "        cv_vals = []\n",
    "        for outer_fold in range(max(spec.outer_cv, 1)):\n",
    "            for rand in range(spec.n_iter):\n",
    "                params = [(\n",
    "                    distr.rvs(random_state=rnd)\n",
    "                    if hasattr(distr, 'rvs')\n",
    "                    else np.random.choice(distr)\n",
    "                ) for par, distr in spec.param_distribution.items()]\n",
    "\n",
    "                for inner_fold in range(spec.inner_cv):\n",
    "                    cv_vals.append((\n",
    "                        outer_fold,\n",
    "                        inner_fold,\n",
    "                        tuple(params),\n",
    "                    ))\n",
    "\n",
    "        if spec.outer_cv > 0:\n",
    "            results = make_nested_cv_tasks(df_bcast, spec, cv_vals)\n",
    "        else:\n",
    "            results = make_cv_tasks(df_bcast, spec, cv_vals)\n",
    "\n",
    "        # we build the result rdd gradually with unions so that we can proceed\n",
    "        # to the outer cvs as soon as the spec's inner cv has finished\n",
    "        # if we built the whole result rdd in one go we would need to wait on\n",
    "        # all inner cvs from all specs to finish, before being able to start\n",
    "        # the outer cvs\n",
    "        if result_rdd is None:\n",
    "            result_rdd = results\n",
    "        else:\n",
    "            result_rdd = result_rdd.union(results)\n",
    "\n",
    "    cv_results = result_rdd.collect()\n",
    "    return cv_results\n",
    "\n",
    "\n",
    "def fit_most_estimator(df, most_only, spark=True):\n",
    "    if most_only:\n",
    "        df = df[(df.zL > -2) & (df.zL < 1)]\n",
    "\n",
    "    dtime = datetime.datetime.utcnow().strftime('%Y%m%d-%H%M%S')\n",
    "    fname = '%s_%smost-%s' % (\n",
    "        'MOSTEstimator', '' if most_only else 'no', dtime\n",
    "    )\n",
    "\n",
    "    meta = {\n",
    "        'model': 'MOSTEstimator',\n",
    "        'most': most_only\n",
    "    }\n",
    "\n",
    "    if spark:\n",
    "        spec = CVSpec(\n",
    "            model=MOSTEstimator(),\n",
    "            param_distribution={'regu': LogUniform(10, -6, 1)},\n",
    "            features=['zL'],\n",
    "            target='phi_m',\n",
    "            inner_cv=10,\n",
    "            outer_cv=10,\n",
    "            n_iter=10,\n",
    "            normalize=False,\n",
    "            meta=meta,\n",
    "            save_to=fname,\n",
    "        )\n",
    "    else:\n",
    "        results, _, _, _ = nested_cv(\n",
    "            df, MOSTEstimator, grid, ['zL'], 'phi_m',\n",
    "            most_only, use_trend=trend,\n",
    "        )\n",
    "        spec = finalize_result(None, results.values, save_to=fname)\n",
    "\n",
    "    if spark:\n",
    "        return nested_cv_spark(df, spec)\n",
    "    else:\n",
    "        return spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_test(df, most_only, outer_cv):\n",
    "    ''' run each model on all features with and without trend\n",
    "    '''\n",
    "        \n",
    "    if most_only:\n",
    "        df = df[(df.zL > -2) & (df.zL < 1)]\n",
    "        \n",
    "    ridge_spec = Ridge, {\n",
    "        'alpha': LogUniform(10, -6, 1)\n",
    "    }\n",
    "    \n",
    "    knn_spec = KNeighborsRegressor, {\n",
    "        'n_neighbors': IntDistribution(LogUniform(10, 0, 1.7)),  # 10**1.7 ~= 50\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'p': [1, 2],\n",
    "    }\n",
    "    \n",
    "    gbr_spec = GradientBoostingRegressor, {\n",
    "        'max_depth': IntDistribution(stats.uniform(loc=1, scale=9)),\n",
    "        'subsample': stats.uniform(loc=0.25, scale=0.75),\n",
    "        'max_features': stats.uniform(0.1, 0.9),\n",
    "        'loss': ['lad', 'ls', 'huber'],\n",
    "        'n_estimators': IntDistribution(LogUniform(10, 2, 3)),\n",
    "        'learning_rate': LogUniform(10, -5, -1),\n",
    "        'alpha': stats.uniform(0.01, 0.98),\n",
    "    }\n",
    "    \n",
    "    repeats = {\n",
    "        (True, 1): 1,\n",
    "        (True, 3): 2,\n",
    "        (False, 4): 3,\n",
    "        (False, 5): 2,\n",
    "    }\n",
    "    \n",
    "    all_specs = []\n",
    "    for trend in [True, False]:\n",
    "        for fset in range(1, 6):\n",
    "            _, features = get_features(df, trend, fset)\n",
    "            for model_cls, grid in [gbr_spec]: # [ridge_spec, knn_spec, gbr_spec]:\n",
    "                for rep in range(repeats.get((trend, fset), 0)):\n",
    "                    _, features = get_features(df, trend, fset)\n",
    "                    dtime = datetime.datetime.utcnow().strftime('%Y%m%d-%H%M%S')\n",
    "                    fname = '%s_f%d_%strend_%smost_%doutercv-%s' % (\n",
    "                        model_cls.__name__, fset,\n",
    "                        '' if trend else 'no',\n",
    "                        '' if most_only else 'no',\n",
    "                        outer_cv, dtime\n",
    "                    )\n",
    "\n",
    "                    all_specs.append(CVSpec(\n",
    "                        model=model_cls(),\n",
    "                        param_distribution=grid,\n",
    "                        features=features,\n",
    "                        target='phi_m',\n",
    "                        inner_cv=10,\n",
    "                        outer_cv=outer_cv,\n",
    "                        n_iter=25,\n",
    "                        normalize=True,\n",
    "                        meta={\n",
    "                            'trend': trend,\n",
    "                            'fset': fset,\n",
    "                            'model': model_cls,\n",
    "                            'most': most_only\n",
    "                        },\n",
    "                        save_to=fname,\n",
    "                    ))\n",
    "\n",
    "    import random\n",
    "    random.shuffle(all_specs)\n",
    "\n",
    "    return spark_cv(df, *all_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = read_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<string>:43: RuntimeWarning: invalid value encountered in divide"
     ]
    }
   ],
   "source": [
    "ddf, _ = get_features(df, use_trend=True, feature_level=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = do_test(ddf, most_only=True, outer_cv=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "global name 'nested_cv_spark' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 542, in fit_most_estimator\n",
      "NameError: global name 'nested_cv_spark' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "most_most_res = fit_most_estimator(ddf, most_only=True, spark=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "global name 'nested_cv_spark' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 542, in fit_most_estimator\n",
      "NameError: global name 'nested_cv_spark' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "most_nomost_res = fit_most_estimator(ddf, most_only=False, spark=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-7c29239dda11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'spark'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu'_ = do_test(ddf, most_only=False, outer_cv=0)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-121>\u001b[0m in \u001b[0;36mspark\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/sparkmagic/livyclientlib/exceptions.pyc\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu\"ENCOUNTERED AN INTERNAL ERROR: {}\\n\\tTraceback:\\n{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_exc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/sparkmagic/livyclientlib/exceptions.pyc\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions_to_handle\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;31m# Do not log! as some messages may contain private client information\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/sparkmagic/kernels/kernelmagics.pyc\u001b[0m in \u001b[0;36mspark\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mcoerce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_coerce_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoerce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_spark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplemethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplefraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoerce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/sparkmagic/magics/sparkmagicsbase.pyc\u001b[0m in \u001b[0;36mexecute_spark\u001b[0;34m(self, cell, output_var, samplemethod, maxrows, samplefraction, session_name, coerce)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexecute_spark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplemethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplefraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoerce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark_controller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipython_display\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/sparkmagic/livyclientlib/sparkcontroller.pyc\u001b[0m in \u001b[0;36mrun_command\u001b[0;34m(self, command, client_name)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0msession_to_use\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_session_by_name_or_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession_to_use\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_sqlquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqlquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/sparkmagic/livyclientlib/command.pyc\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, session)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_statement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mstatement_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mu'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_statement_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             self._spark_events.emit_statement_execution_end_event(session.guid, session.kind, session.id,\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/sparkmagic/livyclientlib/command.pyc\u001b[0m in \u001b[0;36m_get_statement_output\u001b[0;34m(self, session, statement_id)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mFINAL_STATEMENT_STATUS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                 \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m                 \u001b[0mretries\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/sparkmagic/livyclientlib/livysession.pyc\u001b[0m in \u001b[0;36msleep\u001b[0;34m(self, retries)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseconds_to_sleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# This function will refresh the status and get the logs in a single call.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "_ = do_test(ddf, most_only=False, outer_cv=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature importances on F4 with trend and MOST only, check if this makes sense once you try F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     mean       std         imp\n",
      "rain             0.012715  0.000319   39.906414\n",
      "rain_trend       0.014674  0.000254   57.770853\n",
      "temp_20_trend    0.015496  0.000298   51.973389\n",
      "temp_10_trend    0.015859  0.000298   53.249730\n",
      "temp_40_trend    0.016946  0.000542   31.290230\n",
      "temp_20          0.017517  0.000331   52.876421\n",
      "temp_10          0.017995  0.000322   55.843363\n",
      "temp_trend       0.018592  0.000457   40.718311\n",
      "temp_40          0.021400  0.000289   74.118898\n",
      "wind_20_trend    0.022053  0.000320   68.947329\n",
      "temp             0.023832  0.000249   95.614397\n",
      "wind_trend       0.027351  0.000338   80.966239\n",
      "soilheat_trend   0.028076  0.000390   71.923880\n",
      "netrad_trend     0.029169  0.000350   83.305127\n",
      "wind_10_trend    0.030118  0.000562   53.555488\n",
      "wind_40_trend    0.032436  0.000715   45.386153\n",
      "dewpoint_trend   0.033121  0.000399   83.083066\n",
      "soilheat         0.039414  0.000597   66.060931\n",
      "soil_temp_trend  0.040030  0.001077   37.150942\n",
      "z                0.040719  0.000519   78.416001\n",
      "soil_temp        0.043523  0.000747   58.298591\n",
      "dewpoint         0.044244  0.000394  112.225562\n",
      "wind_20          0.056140  0.000549  102.326220\n",
      "wind             0.056759  0.000411  137.934892\n",
      "netrad           0.063500  0.001346   47.165310\n",
      "wind_10          0.118964  0.001129  105.331638\n",
      "wind_40          0.119356  0.001190  100.333543"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    aa, columns=features\n",
    ").describe(\n",
    "    percentiles=[]\n",
    ").T.assign(\n",
    "    imp=lambda df: df['mean'] / df['std']\n",
    ").drop([\n",
    "    'count', 'min', 'max', '50%'\n",
    "], axis=1).sort_values('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          count        mean        std         min  \\\n",
      "explained_variance_score   10.0    0.604488   0.009749    0.588177   \n",
      "mean_absolute_error        10.0    0.395237   0.010928    0.380288   \n",
      "mean_squared_error         10.0    0.332043   0.021671    0.303898   \n",
      "median_absolute_error      10.0    0.282347   0.006470    0.274640   \n",
      "r2_score                   10.0    0.604319   0.009802    0.587802   \n",
      "mean_abs_percent_error     10.0  184.221323  38.776409  113.325184   \n",
      "median_abs_percent_error   10.0   24.624156   1.413938   22.637900   \n",
      "\n",
      "                                 25%         50%         75%         max  \n",
      "explained_variance_score    0.599939    0.602954    0.610194    0.619261  \n",
      "mean_absolute_error         0.388139    0.392386    0.402019    0.416708  \n",
      "mean_squared_error          0.316976    0.324940    0.345071    0.368475  \n",
      "median_absolute_error       0.276705    0.282892    0.284840    0.294774  \n",
      "r2_score                    0.599853    0.602929    0.610082    0.619014  \n",
      "mean_abs_percent_error    169.530949  182.679979  199.873012  263.127400  \n",
      "median_abs_percent_error   23.327171   24.670799   25.868706   26.586394"
     ]
    }
   ],
   "source": [
    "most_res, _, _, _, _ = nested_cv_spark(df, MOSTEstimator(), {\n",
    "    'regu': LogUniform(10, -6, 1)\n",
    "}, 'zL', 'phi_m', normalize=False, n_iter=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
