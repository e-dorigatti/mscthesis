{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we put together things from the nnet and neural architecture search notebooks, and see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import time\n",
    "from scipy.stats import probplot\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from sklearn.utils import shuffle\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, GaussianNoise, Input, PReLU, Activation, Concatenate\n",
    "from keras.initializers import VarianceScaling\n",
    "from keras import regularizers \n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "from sklearn import metrics\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ControllerRNN:\n",
    "    def __init__(self, max_len, batch_size, type_size, arg_size,\n",
    "                 learning_rate=0.001, hidden_size=32, baseline_smoothing=0.95,\n",
    "                 variable_length_nnet=True):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.unroll_by = max_len\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.type_size = type_size + 1 * variable_length_nnet  # 0 is for end-of-network token\n",
    "        self.arg_size = arg_size\n",
    "        self.baseline_smoothing = baseline_smoothing\n",
    "        self.variable_length_nnet = variable_length_nnet\n",
    "\n",
    "    def build(self):\n",
    "        # reward for the architectures\n",
    "        self.architecture_reward = tf.placeholder(tf.float32, [self.batch_size])\n",
    "        \n",
    "        # exponential moving average of the reward\n",
    "        self.last_average_reward = tf.reduce_mean(self.architecture_reward)\n",
    "        self.reward_ema = tf.train.ExponentialMovingAverage(self.baseline_smoothing)\n",
    "        self.update_reward_ema = self.reward_ema.apply([self.last_average_reward])\n",
    "\n",
    "        rnn_input = tf.random_normal([self.batch_size, self.type_size + self.arg_size])\n",
    "        rnn = tf.contrib.rnn.GRUCell(self.hidden_size)\n",
    "        state = tf.random_normal([self.batch_size, rnn.state_size])\n",
    "\n",
    "        # weight matrices to transform from rnn output to layer type and discrete arg\n",
    "        rnn_to_layer_type_weight = tf.Variable(tf.random_normal([rnn.output_size + 1, self.type_size]))\n",
    "        rnn_to_layer_type_gradient = []\n",
    "        rnn_to_layer_arg_weight = tf.Variable(tf.random_normal([rnn.output_size + 1, self.arg_size]))\n",
    "        rnn_to_layer_arg_gradient = []\n",
    "\n",
    "        # layer_probs contains the output from the network, namely the probabilities\n",
    "        # of type and argument for every layer of every network\n",
    "        self.layer_probs = []\n",
    "\n",
    "        # layer_indicators contains one-hot indicators of type and argument\n",
    "        # for every layer of every network.\n",
    "        # used to select which action is used to compute the gradient.\n",
    "        # fixed, must be set before updating the weights\n",
    "        self.layer_indicators = []\n",
    "\n",
    "        losses = []\n",
    "        for i in range(self.unroll_by):\n",
    "            # run rnn cell\n",
    "            output, state = rnn(rnn_input, state)\n",
    "\n",
    "            if i == 0:  # rnn variables are only initialized now\n",
    "                rnn_params = rnn.trainable_variables + rnn.trainable_weights\n",
    "                rnn_gradients = [[] for _ in range(len(rnn_params))]\n",
    "\n",
    "            # compute output probabilites\n",
    "            output = tf.concat([output, tf.ones((output.shape[0], 1))], axis=1)\n",
    "            layer_type = tf.nn.relu(tf.matmul(output, rnn_to_layer_type_weight))\n",
    "            layer_arg = tf.nn.relu(tf.matmul(output, rnn_to_layer_arg_weight))\n",
    "            rnn_input = tf.concat([layer_type, layer_arg], axis=1)\n",
    "\n",
    "            layer_type_probs = tf.nn.softmax(layer_type)\n",
    "            layer_arg_probs = tf.nn.softmax(layer_arg)\n",
    "\n",
    "            chosen_layer_type = tf.placeholder(tf.int32, self.batch_size)\n",
    "            chosen_layer_arg = tf.placeholder(tf.int32, self.batch_size)\n",
    "\n",
    "            self.layer_probs.append((layer_type_probs, layer_arg_probs))\n",
    "            self.layer_indicators.append((chosen_layer_type, chosen_layer_arg))\n",
    "\n",
    "            # aggregate gradients\n",
    "            baseline = self.reward_ema.average(self.last_average_reward)\n",
    "            prob = (self.last_average_reward - baseline) * (\n",
    "                tf.reduce_sum(\n",
    "                    tf.one_hot(\n",
    "                        chosen_layer_type, depth=self.type_size\n",
    "                    ) * tf.log(layer_type_probs + 1e-12),\n",
    "                    axis=1\n",
    "                ) + tf.reduce_sum(\n",
    "                    tf.one_hot(\n",
    "                        chosen_layer_arg, depth=self.arg_size\n",
    "                    ) * tf.log(layer_arg_probs + 1e-12),\n",
    "                    axis=1\n",
    "                )\n",
    "            )\n",
    "            losses.append(prob)\n",
    "\n",
    "            rnn_to_layer_arg_gradient.append(tf.gradients(prob, rnn_to_layer_arg_weight)[0])\n",
    "            rnn_to_layer_type_gradient.append(tf.gradients(prob, rnn_to_layer_type_weight)[0])\n",
    "            for param, grad in zip(rnn_params, rnn_gradients):\n",
    "                grad.append(tf.gradients(prob, param)[0])\n",
    "\n",
    "        self.loss = tf.reduce_mean(losses)\n",
    "\n",
    "        def sanitize_gradient(grads):\n",
    "            avg = sum(grads) / len(grads)\n",
    "            return tf.clip_by_norm(avg, 1.0)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self.optimize = optimizer.apply_gradients([\n",
    "            (sanitize_gradient(grad), param)\n",
    "            for param, grad in zip(rnn_params, rnn_gradients)\n",
    "        ] + [\n",
    "            (sanitize_gradient(rnn_to_layer_type_gradient), rnn_to_layer_type_weight),\n",
    "            (sanitize_gradient(rnn_to_layer_arg_gradient), rnn_to_layer_arg_weight),\n",
    "        ])\n",
    "\n",
    "    def generate_architecture(self, session):\n",
    "        layers = session.run(self.layer_probs)\n",
    "        networks = [[] for _ in range(self.batch_size)]\n",
    "        for (ltype, larg) in layers:\n",
    "            for i, (nnet, type_prob, arg_prob) in enumerate(zip(networks, ltype, larg)):\n",
    "                if self.variable_length_nnet and nnet and nnet[-1][0] == 0:\n",
    "                    continue\n",
    "\n",
    "                assert all(np.isfinite(type_prob))\n",
    "                assert all(np.isfinite(arg_prob))\n",
    "\n",
    "                layer_type = np.random.choice(len(type_prob), p=type_prob)\n",
    "                layer_arg = np.random.choice(len(arg_prob), p=arg_prob)\n",
    "\n",
    "                nnet.append((layer_type, layer_arg))\n",
    "\n",
    "        return networks\n",
    "\n",
    "    def learn_from_rewards(self, sess, networks, rewards):\n",
    "        assert len(rewards) == self.batch_size\n",
    "\n",
    "        # set the indicator variables, telling which action was chosen\n",
    "        feed_dict = {ind: []\n",
    "                     for layer_ind in self.layer_indicators\n",
    "                     for ind in layer_ind}\n",
    "\n",
    "        for nnet in networks:\n",
    "            # pad network if shorter than expected\n",
    "            # we set the indicators to -1, so that all one hot will be 0\n",
    "            # thus not contributing to the gradient\n",
    "            if len(nnet) < self.unroll_by:\n",
    "                nnet = nnet + [(-1, -1)] * (self.unroll_by - len(nnet))\n",
    "\n",
    "            assert len(nnet) == self.unroll_by\n",
    "            for (itype, iarg), (ntype, narg) in zip(self.layer_indicators, nnet):\n",
    "                feed_dict[itype].append(ntype)\n",
    "                feed_dict[iarg].append(narg)\n",
    "\n",
    "        feed_dict[self.architecture_reward] = rewards\n",
    "        loss, _, _ = sess.run([self.loss, self.update_reward_ema, self.optimize],\n",
    "                              feed_dict=feed_dict)\n",
    "        return loss\n",
    "\n",
    "    \n",
    "class MovingAverages:\n",
    "    def __init__(self):\n",
    "        self.metrics = {}\n",
    "        self.smoothing = {}\n",
    "        self.snapshots = []\n",
    "    \n",
    "    def update(self, metric, value, smoothing=None):\n",
    "        if smoothing is None:\n",
    "            smoothing = self.smoothing.get(metric, 0.6)\n",
    "        self.smoothing[metric] = smoothing\n",
    "        \n",
    "        # can pass None to update smoothing\n",
    "        if value is not None:\n",
    "            self.metrics[metric] = (\n",
    "                smoothing * self.metrics.get(metric, value)\n",
    "                + (1 - smoothing) * value\n",
    "            )\n",
    "        return self.metrics[metric]\n",
    "    \n",
    "    def update_all(self, **metrics):\n",
    "        for metric, value in metrics.items():\n",
    "            self.update(metric, value)\n",
    "        return [self.metrics[m] for m in metrics]\n",
    "    \n",
    "    def snapshot(self, **meta):\n",
    "        snap = dict(self.metrics)\n",
    "        snap.update(meta)\n",
    "        self.snapshots.append(snap)\n",
    "        return snap\n",
    "\n",
    "    \n",
    "def make_index(dtimes, interval):\n",
    "    # returns a tuple index_above, index_below\n",
    "    # index_above[i] is the largest i\n",
    "    # such that dtimes[index_above[i]] - dtimes[i] < interval\n",
    "    # index_below[i] is the smallest i\n",
    "    # such that dtimes[i] - dtimes[index_below[i]] < interval\n",
    "    # dtimes must be already sorted!\n",
    "    index_below, index_above = np.zeros(\n",
    "        (2, len(dtimes)), dtype=np.int\n",
    "    ) - 1\n",
    "    \n",
    "    for i, x in enumerate(dtimes):\n",
    "        j = index_below[i - 1] if i > 0 else 0\n",
    "        while x - dtimes[j] > interval:\n",
    "            j += 1\n",
    "\n",
    "        index_below[i] = j\n",
    "        index_above[j] = i\n",
    "\n",
    "    last_above = index_above[0]\n",
    "    for i in range(len(dtimes)):\n",
    "        if index_above[i] < 0:\n",
    "            index_above[i] = last_above\n",
    "        else:\n",
    "            last_above = index_above[i]\n",
    "    \n",
    "    return index_above, index_below\n",
    "\n",
    "\n",
    "def compute_trend(df, columns, interval=3600):\n",
    "    df = df.sort_values('datetime')\n",
    "    for z in df.z.unique():  \n",
    "        this_level = df[df.z == z]\n",
    "        index_above, index_below = make_index(this_level.datetime.values, interval)\n",
    "\n",
    "        for col in columns:\n",
    "            val_above = this_level[col].values\n",
    "            val_below = this_level.iloc[index_below][col].values\n",
    "\n",
    "            time_above = this_level.datetime.values\n",
    "            time_below = this_level.iloc[index_below].datetime.values\n",
    "\n",
    "            trend = 3600 * (val_above - val_below) / (time_above - time_below)\n",
    "\n",
    "            df.loc[df.z == z, col + '_trend'] = trend\n",
    "\n",
    "    return df, [col + '_trend' for col in columns]\n",
    "\n",
    "\n",
    "def get_features(df, use_trend, feature_level):\n",
    "    wind_temp_levels = df.pivot_table(\n",
    "        values=['wind', 'temp'], columns='z', index=['ds', 'tt']\n",
    "    ).reset_index()\n",
    "    wind_temp_levels.columns = [\n",
    "        '%s_%d' % (a, b) if b else a\n",
    "        for a, b in wind_temp_levels.columns.values\n",
    "    ]\n",
    "\n",
    "    df = df.merge(wind_temp_levels, on=['ds', 'tt'])\n",
    "\n",
    "    feature_sets = [\n",
    "        [\n",
    "            'z', 'wind', 'temp', 'soil_temp',\n",
    "            'wind_10', 'wind_20', 'wind_40',\n",
    "            'temp_10', 'temp_20', 'temp_40',\n",
    "        ],\n",
    "        ['soilheat'],\n",
    "        ['netrad'],\n",
    "        ['rain', 'dewpoint'],\n",
    "        ['H', 'LE'],\n",
    "    ]\n",
    "\n",
    "    features = [\n",
    "        f for fset in feature_sets[:feature_level]\n",
    "        for f in fset\n",
    "    ]\n",
    "    \n",
    "    if use_trend:\n",
    "        df, added_cols = compute_trend(df, [\n",
    "            f for f in features if f != 'z'\n",
    "        ])\n",
    "        features.extend(added_cols)\n",
    "\n",
    "    return df, features\n",
    "\n",
    "\n",
    "def get_train_test_data(df, features, target, samples_count, n_months=12):\n",
    "    df = df.dropna()\n",
    "\n",
    "    # get random test months\n",
    "    test_ds = np.random.choice(df.ds.unique(), n_months, replace=False)\n",
    "    test_mask = df.ds.isin(test_ds)\n",
    "    \n",
    "    train_df, test_df = df.loc[~test_mask], df.loc[test_mask]\n",
    "    if samples_count > 0:\n",
    "        # maintain proportion of train/test samples\n",
    "        test_size = int(samples_count * len(test_df) / len(train_df))\n",
    "        train_df = train_df.sample(samples_count)\n",
    "        test_df = test_df.sample(test_size)\n",
    "    \n",
    "    train_x, train_y = train_df[features], train_df[target]\n",
    "    test_x, test_y = test_df[features], test_df[target]\n",
    "\n",
    "    mean_x, mean_y = train_x.mean(), train_y.mean()\n",
    "    std_x, std_y = train_x.std(), train_y.std()\n",
    "\n",
    "    train_x = (train_x - mean_x) /  std_x\n",
    "    test_x = (test_x - mean_x) / std_x\n",
    "    \n",
    "    assert np.all(np.isfinite(train_x))\n",
    "    \n",
    "    train_y = (train_y - mean_y) / std_y\n",
    "    test_y = (test_y - mean_y) / std_y\n",
    "\n",
    "    return train_x, train_y, test_x, test_y, mean_y, std_y\n",
    "    \n",
    "\n",
    "def compute_denormalized_mse(std_y):\n",
    "    def denormalized_mse(y_true, y_pred):\n",
    "        # model is trained with normalized data, but we want\n",
    "        # mse on not normalized data to compare with MOST\n",
    "        mse = K.mean(K.square(y_true - y_pred), axis=-1)\n",
    "        return mse * std_y**2\n",
    "    return denormalized_mse\n",
    "\n",
    "\n",
    "def build_model(input_shape, architecture, std_y=1):\n",
    "    # build model with fixed length architecture\n",
    "    layers = [Input(shape=(input_shape,))]\n",
    "\n",
    "    for i, (layer_type, layer_arg) in enumerate(architecture):\n",
    "        if i % 2 == 0:\n",
    "            num = 2**layer_arg\n",
    "            layers.append(PReLU()(\n",
    "                    Dense(num, kernel_initializer=VarianceScaling(2, 'fan_in'))(\n",
    "                        layers[-1]\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            pkeep = (layer_arg + 1) / 10  # from 0.1 to 1\n",
    "            if pkeep < 1:\n",
    "                layers.append(Dropout(pkeep)(layers[-1]))\n",
    "\n",
    "    layers.append(Dense(1)(layers[-1]))\n",
    "\n",
    "    opt = Adam(lr=0.001)\n",
    "    model = Model(inputs=layers[0], outputs=layers[-1])\n",
    "    model.compile(loss='mse', optimizer=opt, metrics=[compute_denormalized_mse(std_y)])\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model_vlen(input_shape, architecture, std_y=1):\n",
    "    # build model with variable length architecture\n",
    "    layers = [Input(shape=(input_shape,))]\n",
    "\n",
    "    for layer_type, layer_arg in architecture:\n",
    "        if layer_type == 0 or layer_type == 1:\n",
    "            num = 2**layer_arg\n",
    "            layers.append(PReLU()(\n",
    "                    Dense(num, kernel_initializer=VarianceScaling(2, 'fan_in'))(\n",
    "                        layers[-1]\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        elif layer_type == 2:\n",
    "            pkeep = (layer_arg + 1) / 11  # from 1/11 to 10/11\n",
    "            layers.append(Dropout(pkeep)(layers[-1]))\n",
    "        else:\n",
    "            raise ValueError('layer type from 0 to 2')\n",
    "\n",
    "    layers.append(Dense(1)(layers[-1]))\n",
    "\n",
    "    opt = Adam(lr=0.001)\n",
    "    model = Model(inputs=layers[0], outputs=layers[-1])\n",
    "    model.compile(loss='mse', optimizer=opt, metrics=[compute_denormalized_mse(std_y)])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_architecture(step, arch_idx, architecture, max_epochs, samples_count):\n",
    "    train_x, train_y, test_x, test_y, _, std_y = get_train_test_data(\n",
    "        ddf, features, 'phi_m', samples_count, n_months=12\n",
    "    )\n",
    "    \n",
    "    #K.clear_session()  # https://stackoverflow.com/q/35114376/521776\n",
    "    model = build_model(train_x.shape[1], architecture, std_y=std_y)\n",
    "\n",
    "    logdir = 'dev/logs/nas-2/step-%d-arch-%d' % (step, arch_idx)\n",
    "    callbacks = [\n",
    "        ReduceLROnPlateau(factor=0.1, verbose=0, min_lr=1e-6, patience=10, monitor='loss'),\n",
    "        TensorBoard(logdir, write_graph=True, write_grads=True, histogram_freq=0),\n",
    "        EarlyStopping(min_delta=0.001, patience=25),\n",
    "    ]\n",
    "\n",
    "    hist = model.fit(\n",
    "        train_x, train_y,\n",
    "        batch_size=1024,\n",
    "        epochs=max_epochs,\n",
    "        verbose=0,\n",
    "        shuffle=True,\n",
    "        callbacks=callbacks,\n",
    "        validation_data=(test_x, test_y)\n",
    "    )\n",
    "\n",
    "    best = min(hist.history['val_denormalized_mse'])\n",
    "\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/ipykernel_launcher.py:221: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    dframe_path = 'data/cabauw/processed-full-log.csv.gz'\n",
    "    df = pd.read_csv(dframe_path, na_values='--', compression='gzip')\n",
    "\n",
    "    df = df[(df.ustar > 0.1) & (abs(df.H) > 10) & (df.wind > 1)]\n",
    "    df = df[df.ds != 201603]\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = load_data()\n",
    "ddf, features = get_features(df, use_trend=True, feature_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "controller = ControllerRNN(\n",
    "    hidden_size=64,\n",
    "    max_len=16,\n",
    "    batch_size=1,\n",
    "    type_size=1,\n",
    "    arg_size=10,\n",
    "    learning_rate=0.001,\n",
    "    baseline_smoothing=0.99,\n",
    "    variable_length_nnet=False\n",
    ")\n",
    "\n",
    "hist = []\n",
    "controller_graph = tf.Graph()\n",
    "with controller_graph.as_default():\n",
    "    controller.build()\n",
    "    controller_session = tf.Session(graph=controller_graph)\n",
    "    controller_session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "np.random.seed(4312)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contro_loss=8.213  inner_mse=1.536  step=0.000  time=29.399\n",
      "[[(0, 0), (0, 3), (0, 3), (0, 3), (0, 3), (0, 3), (0, 3), (0, 3), (0, 3), (0, 3), (0, 3), (0, 3), (0, 3), (0, 3), (0, 3), (0, 3)]]\n",
      "contro_loss=19.503  inner_mse=1.872  step=1.000  time=374.429\n",
      "[[(0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8)]]\n",
      "contro_loss=51.795  inner_mse=2.001  step=2.000  time=553.970\n",
      "[[(0, 4), (0, 4), (0, 4), (0, 4), (0, 1), (0, 1), (0, 1), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8)]]\n",
      "contro_loss=45.309  inner_mse=1.905  step=3.000  time=624.190\n",
      "[[(0, 9), (0, 9), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1)]]\n",
      "contro_loss=27.561  inner_mse=1.942  step=4.000  time=651.273\n",
      "[[(0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 3), (0, 1), (0, 1)]]\n",
      "contro_loss=19.982  inner_mse=1.478  step=5.000  time=719.394\n",
      "[[(0, 2), (0, 2), (0, 2), (0, 2), (0, 6), (0, 6), (0, 2), (0, 2), (0, 2), (0, 2), (0, 6), (0, 2), (0, 6), (0, 6), (0, 6), (0, 6)]]\n",
      "contro_loss=82.268  inner_mse=1.758  step=6.000  time=1034.757\n",
      "[[(0, 6), (0, 6), (0, 6), (0, 6), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8)]]\n",
      "contro_loss=182.544  inner_mse=1.951  step=7.000  time=1426.187\n",
      "[[(0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8)]]\n",
      "contro_loss=110.517  inner_mse=2.007  step=8.000  time=1454.391\n",
      "[[(0, 0), (0, 0), (0, 0), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1)]]\n",
      "contro_loss=66.482  inner_mse=1.778  step=9.000  time=1568.053\n",
      "[[(0, 6), (0, 5), (0, 9), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 6), (0, 1), (0, 1), (0, 6), (0, 1), (0, 1), (0, 1), (0, 1)]]\n",
      "contro_loss=40.161  inner_mse=2.101  step=10.000  time=1595.559\n",
      "[[(0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1)]]\n",
      "contro_loss=53.116  inner_mse=1.988  step=11.000  time=1623.641\n",
      "[[(0, 3), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 6), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1)]]\n",
      "contro_loss=80.016  inner_mse=2.010  step=12.000  time=1652.191\n",
      "[[(0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1)]]\n",
      "contro_loss=48.010  inner_mse=1.796  step=13.000  time=1680.089\n",
      "[[(0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1)]]\n",
      "contro_loss=120.098  inner_mse=1.903  step=14.000  time=1707.897\n",
      "[[(0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1)]]\n",
      "contro_loss=72.230  inner_mse=1.645  step=15.000  time=1795.608\n",
      "[[(0, 9), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1)]]\n",
      "contro_loss=127.054  inner_mse=1.795  step=16.000  time=1879.499\n",
      "[[(0, 0), (0, 0), (0, 9), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1)]]\n",
      "contro_loss=140.297  inner_mse=1.904  step=17.000  time=1911.942\n",
      "[[(0, 3), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 3), (0, 3), (0, 3), (0, 3), (0, 3), (0, 3), (0, 3), (0, 3), (0, 3), (0, 3)]]\n",
      "contro_loss=91.473  inner_mse=1.853  step=18.000  time=1941.673\n",
      "[[(0, 2), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 1), (0, 1), (0, 1), (0, 3), (0, 3), (0, 3), (0, 3), (0, 3), (0, 3), (0, 3)]]\n",
      "contro_loss=55.645  inner_mse=2.011  step=19.000  time=2047.648\n",
      "[[(0, 6), (0, 6), (0, 6), (0, 6), (0, 6), (0, 6), (0, 6), (0, 6), (0, 6), (0, 6), (0, 6), (0, 6), (0, 6), (0, 6), (0, 6), (0, 6)]]\n",
      "contro_loss=36.777  inner_mse=1.591  step=20.000  time=2080.053\n",
      "[[(0, 1), (0, 1), (0, 1), (0, 1), (0, 3), (0, 3), (0, 3), (0, 3), (0, 3), (0, 3), (0, 3), (0, 3), (0, 3), (0, 3), (0, 3), (0, 3)]]\n",
      "contro_loss=84.727  inner_mse=1.830  step=21.000  time=2213.366\n",
      "[[(0, 9), (0, 9), (0, 6), (0, 6), (0, 6), (0, 1), (0, 1), (0, 1), (0, 6), (0, 6), (0, 6), (0, 6), (0, 6), (0, 6), (0, 6), (0, 6)]]\n",
      "contro_loss=50.862  inner_mse=2.049  step=22.000  time=2559.835\n",
      "[[(0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8)]]\n",
      "contro_loss=30.517  inner_mse=1.721  step=23.000  time=2587.178\n",
      "[[(0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1)]]\n",
      "contro_loss=69.499  inner_mse=1.948  step=24.000  time=2658.751\n",
      "[[(0, 5), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 6), (0, 6), (0, 6), (0, 6), (0, 6), (0, 6), (0, 6), (0, 6), (0, 6)]]\n",
      "contro_loss=101.357  inner_mse=2.074  step=25.000  time=2934.687\n",
      "[[(0, 5), (0, 5), (0, 5), (0, 2), (0, 2), (0, 2), (0, 8), (0, 8), (0, 8), (0, 7), (0, 7), (0, 7), (0, 9), (0, 9), (0, 9), (0, 9)]]\n",
      "contro_loss=72.420  inner_mse=2.181  step=26.000  time=3107.439\n",
      "[[(0, 6), (0, 2), (0, 2), (0, 2), (0, 2), (0, 8), (0, 8), (0, 8), (0, 2), (0, 2), (0, 8), (0, 2), (0, 8), (0, 8), (0, 8), (0, 8)]]\n",
      "contro_loss=43.483  inner_mse=2.212  step=27.000  time=3456.023\n",
      "[[(0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8)]]\n",
      "contro_loss=26.127  inner_mse=1.789  step=28.000  time=3485.798\n",
      "[[(0, 4), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 3), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1)]]\n",
      "contro_loss=28.402  inner_mse=1.429  step=29.000  time=3746.893\n",
      "[[(0, 5), (0, 9), (0, 3), (0, 1), (0, 6), (0, 6), (0, 9), (0, 6), (0, 6), (0, 9), (0, 9), (0, 9), (0, 9), (0, 9), (0, 1), (0, 9)]]\n",
      "contro_loss=48.671  inner_mse=1.731  step=30.000  time=3903.179\n",
      "[[(0, 8), (0, 8), (0, 7), (0, 8), (0, 6), (0, 8), (0, 6), (0, 6), (0, 8), (0, 6), (0, 6), (0, 7), (0, 7), (0, 7), (0, 3), (0, 3)]]\n",
      "contro_loss=32.973  inner_mse=1.791  step=31.000  time=3943.954\n",
      "[[(0, 4), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 6), (0, 6), (0, 1), (0, 1), (0, 9), (0, 1), (0, 1), (0, 1), (0, 6)]]\n",
      "contro_loss=61.690  inner_mse=1.986  step=32.000  time=4050.175\n",
      "[[(0, 6), (0, 6), (0, 6), (0, 6), (0, 6), (0, 6), (0, 6), (0, 6), (0, 6), (0, 6), (0, 6), (0, 6), (0, 6), (0, 6), (0, 6), (0, 6)]]\n",
      "contro_loss=46.319  inner_mse=1.609  step=33.000  time=4178.008\n",
      "[[(0, 7), (0, 7), (0, 7), (0, 7), (0, 7), (0, 9), (0, 9), (0, 9), (0, 1), (0, 9), (0, 1), (0, 9), (0, 1), (0, 1), (0, 1), (0, 1)]]\n",
      "contro_loss=45.795  inner_mse=1.760  step=34.000  time=4596.799\n",
      "[[(0, 9), (0, 9), (0, 9), (0, 1), (0, 1), (0, 9), (0, 9), (0, 9), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8)]]\n",
      "contro_loss=27.686  inner_mse=2.066  step=35.000  time=4924.792\n",
      "[[(0, 6), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8)]]\n",
      "contro_loss=23.557  inner_mse=2.073  step=36.000  time=5282.434\n",
      "[[(0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 9), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8)]]\n",
      "contro_loss=16.564  inner_mse=2.288  step=37.000  time=5620.524\n",
      "[[(0, 6), (0, 3), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8), (0, 8)]]\n"
     ]
    }
   ],
   "source": [
    "averages = MovingAverages()\n",
    "averages.smoothing['time'] = 0\n",
    "start_time = time.time()\n",
    "for step_idx in range(10000):\n",
    "    avg_mse = averages.metrics.get('inner_mse', 10)\n",
    "    if avg_mse > 1:\n",
    "        max_epochs = 5\n",
    "    elif avg_mse > 0.5:\n",
    "        max_epochs = 20\n",
    "    elif avg_mse > 0.3:\n",
    "        max_epochs = 50\n",
    "    else:\n",
    "        max_epochs = 500\n",
    "\n",
    "    architectures = controller.generate_architecture(controller_session)\n",
    "\n",
    "    # test architectures on a temporary graph\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.Session().as_default():\n",
    "            rewards = [\n",
    "                evaluate_architecture(\n",
    "                    step_idx, arch_idx, arch,\n",
    "                    max_epochs, samples_count=-1\n",
    "                )\n",
    "                for arch_idx, arch in enumerate(architectures)\n",
    "            ]\n",
    "\n",
    "    loss = controller.learn_from_rewards(\n",
    "        controller_session, architectures, rewards\n",
    "    )\n",
    "\n",
    "    averages.update_all(\n",
    "        contro_loss=loss**2,\n",
    "        inner_mse=np.mean(rewards),\n",
    "    )\n",
    "\n",
    "    if step_idx % 1 == 0:\n",
    "        snap = averages.snapshot(step=step_idx, time=time.time() - start_time)\n",
    "        print('  '.join(\n",
    "            '%s=%.3f' % metric for metric in snap.items()\n",
    "        ))\n",
    "        print(architectures)\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
