{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we put together things from the nnet and neural architecture search notebooks, and see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import time\n",
    "from scipy.stats import probplot\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from sklearn.utils import shuffle\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, GaussianNoise, Input, PReLU, Activation, Concatenate\n",
    "from keras.initializers import VarianceScaling\n",
    "from keras import regularizers \n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "from sklearn import metrics\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ControllerRNN:\n",
    "    def __init__(self, max_len, batch_size, type_size, arg_size,\n",
    "                 learning_rate=0.001, hidden_size=32, baseline_smoothing=0.95):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.unroll_by = max_len\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.type_size = type_size + 1  # 0 is for end-of-network token\n",
    "        self.arg_size = arg_size\n",
    "        self.baseline_smoothing = baseline_smoothing\n",
    "\n",
    "    def build(self):\n",
    "        # reward for the architectures\n",
    "        self.architecture_reward = tf.placeholder(tf.float32, [self.batch_size])\n",
    "        \n",
    "        # exponential moving average of the reward\n",
    "        self.last_average_reward = tf.reduce_mean(self.architecture_reward)\n",
    "        self.reward_ema = tf.train.ExponentialMovingAverage(self.baseline_smoothing)\n",
    "        self.update_reward_ema = self.reward_ema.apply([self.last_average_reward])\n",
    "\n",
    "        rnn = tf.contrib.rnn.GRUCell(self.hidden_size)\n",
    "        state = tf.random_normal([self.batch_size, rnn.state_size])\n",
    "        \n",
    "        # weight matrices to transform from rnn output to layer type and discrete arg\n",
    "        rnn_to_layer_type_weight = tf.Variable(tf.random_normal([rnn.output_size, self.type_size]))\n",
    "        rnn_to_layer_type_gradient = []\n",
    "        rnn_to_layer_arg_weight = tf.Variable(tf.random_normal([rnn.output_size, self.arg_size]))\n",
    "        rnn_to_layer_arg_gradient = []\n",
    "\n",
    "        # rnn output and gradients\n",
    "        output = tf.random_normal([self.batch_size, rnn.output_size])\n",
    "\n",
    "        # layer_probs contains the output from the network, namely the probabilities\n",
    "        # of type and argument for every layer of every network\n",
    "        self.layer_probs = []\n",
    "\n",
    "        # layer_indicators contains one-hot indicators of type and argument\n",
    "        # for every layer of every network.\n",
    "        # used to select which action is used to compute the gradient.\n",
    "        # fixed, must be set before updating the weights\n",
    "        self.layer_indicators = []\n",
    "\n",
    "        losses = []\n",
    "        \n",
    "        for i in range(self.unroll_by):\n",
    "            # run rnn cell\n",
    "            output, state = rnn(output, state)\n",
    "\n",
    "            if i == 0:  # gru variables are only initialized now\n",
    "                rnn_params = rnn.trainable_variables + rnn.trainable_weights\n",
    "                rnn_gradients = [[] for _ in range(len(rnn_params))]\n",
    "\n",
    "            # compute output probabilites\n",
    "            layer_type = tf.nn.softmax(tf.matmul(output, rnn_to_layer_type_weight))\n",
    "            layer_arg = tf.nn.softmax(tf.matmul(output, rnn_to_layer_arg_weight))\n",
    "            chosen_layer_type = tf.placeholder(tf.int32, self.batch_size)\n",
    "            chosen_layer_arg = tf.placeholder(tf.int32, self.batch_size)\n",
    "            self.layer_probs.append((layer_type, layer_arg))\n",
    "            self.layer_indicators.append((chosen_layer_type, chosen_layer_arg))\n",
    "\n",
    "            # aggregate gradients\n",
    "            baseline = self.reward_ema.average(self.last_average_reward)\n",
    "            prob = (self.last_average_reward - baseline) * (\n",
    "                tf.reduce_sum(\n",
    "                    tf.one_hot(chosen_layer_type, depth=self.type_size) * tf.log(layer_type + 1e-12),\n",
    "                    axis=1\n",
    "                ) + tf.reduce_sum(\n",
    "                    tf.one_hot(chosen_layer_arg, depth=self.arg_size) * tf.log(layer_arg + 1e-12),\n",
    "                    axis=1\n",
    "                )\n",
    "            )\n",
    "            losses.append(prob)\n",
    "\n",
    "            rnn_to_layer_arg_gradient.append(tf.gradients(prob, rnn_to_layer_arg_weight)[0])\n",
    "            rnn_to_layer_type_gradient.append(tf.gradients(prob, rnn_to_layer_type_weight)[0])\n",
    "            for param, grad in zip(rnn_params, rnn_gradients):\n",
    "                grad.append(tf.gradients(prob, param)[0])\n",
    "\n",
    "        self.loss = tf.reduce_mean(losses)\n",
    "\n",
    "        def sanitize_gradient(grads):\n",
    "            avg = sum(grads) / len(grads)\n",
    "            return tf.clip_by_norm(avg, 1.0)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self.optimize = optimizer.apply_gradients([\n",
    "            (sanitize_gradient(grad), param)\n",
    "            for param, grad in zip(rnn_params, rnn_gradients)\n",
    "        ] + [\n",
    "            (sanitize_gradient(rnn_to_layer_type_gradient), rnn_to_layer_type_weight),\n",
    "            (sanitize_gradient(rnn_to_layer_arg_gradient), rnn_to_layer_arg_weight),\n",
    "        ])\n",
    "\n",
    "    def generate_architecture(self, session):\n",
    "        layers = session.run(self.layer_probs)\n",
    "        networks = [[] for _ in range(self.batch_size)]\n",
    "        for (ltype, larg) in layers:\n",
    "            for i, (nnet, type_prob, arg_prob) in enumerate(zip(networks, ltype, larg)):\n",
    "                # the network always has at least one layer\n",
    "                if nnet and nnet[-1][0] == 0:\n",
    "                    continue\n",
    "\n",
    "                assert all(np.isfinite(type_prob))\n",
    "                assert all(np.isfinite(arg_prob))\n",
    "\n",
    "                layer_type = np.random.choice(len(type_prob), p=type_prob)\n",
    "                layer_arg = np.random.choice(len(arg_prob), p=arg_prob)\n",
    "                nnet.append((layer_type, layer_arg))\n",
    "\n",
    "        return networks\n",
    "\n",
    "    def learn_from_rewards(self, sess, networks, rewards):\n",
    "        assert len(rewards) == self.batch_size\n",
    "\n",
    "        # set the indicator variables, telling which action was chosen\n",
    "        feed_dict = {ind: []\n",
    "                     for layer_ind in self.layer_indicators\n",
    "                     for ind in layer_ind}\n",
    "\n",
    "        for nnet in networks:\n",
    "            # pad network if shorter than expected\n",
    "            # we set the indicators to -1, so that all one hot will be 0\n",
    "            # thus not contributing to the gradient\n",
    "            if len(nnet) < self.unroll_by:\n",
    "                nnet = nnet + [(-1, -1)] * (self.unroll_by - len(nnet))\n",
    "\n",
    "            assert len(nnet) == self.unroll_by\n",
    "            for (itype, iarg), (ntype, narg) in zip(self.layer_indicators, nnet):\n",
    "                feed_dict[itype].append(ntype)\n",
    "                feed_dict[iarg].append(narg)\n",
    "\n",
    "        feed_dict[self.architecture_reward] = rewards\n",
    "        loss, _, _ = sess.run([self.loss, self.update_reward_ema, self.optimize],\n",
    "                              feed_dict=feed_dict)\n",
    "        return loss\n",
    "\n",
    "    \n",
    "class MovingAverages:\n",
    "    def __init__(self):\n",
    "        self.metrics = {}\n",
    "        self.smoothing = {}\n",
    "        self.snapshots = []\n",
    "    \n",
    "    def update(self, metric, value, smoothing=None):\n",
    "        if smoothing is None:\n",
    "            smoothing = self.smoothing.get(metric, 0.6)\n",
    "        self.smoothing[metric] = smoothing\n",
    "        \n",
    "        # can pass None to update smoothing\n",
    "        if value is not None:\n",
    "            self.metrics[metric] = (\n",
    "                smoothing * self.metrics.get(metric, value)\n",
    "                + (1 - smoothing) * value\n",
    "            )\n",
    "        return self.metrics[metric]\n",
    "    \n",
    "    def update_all(self, **metrics):\n",
    "        for metric, value in metrics.items():\n",
    "            self.update(metric, value)\n",
    "        return [self.metrics[m] for m in metrics]\n",
    "    \n",
    "    def snapshot(self, **meta):\n",
    "        snap = dict(self.metrics)\n",
    "        snap.update(meta)\n",
    "        self.snapshots.append(snap)\n",
    "        return snap\n",
    "\n",
    "    \n",
    "def make_index(dtimes, interval):\n",
    "    # returns a tuple index_above, index_below\n",
    "    # index_above[i] is the largest i\n",
    "    # such that dtimes[index_above[i]] - dtimes[i] < interval\n",
    "    # index_below[i] is the smallest i\n",
    "    # such that dtimes[i] - dtimes[index_below[i]] < interval\n",
    "    # dtimes must be already sorted!\n",
    "    index_below, index_above = np.zeros(\n",
    "        (2, len(dtimes)), dtype=np.int\n",
    "    ) - 1\n",
    "    \n",
    "    for i, x in enumerate(dtimes):\n",
    "        j = index_below[i - 1] if i > 0 else 0\n",
    "        while x - dtimes[j] > interval:\n",
    "            j += 1\n",
    "\n",
    "        index_below[i] = j\n",
    "        index_above[j] = i\n",
    "\n",
    "    last_above = index_above[0]\n",
    "    for i in range(len(dtimes)):\n",
    "        if index_above[i] < 0:\n",
    "            index_above[i] = last_above\n",
    "        else:\n",
    "            last_above = index_above[i]\n",
    "    \n",
    "    return index_above, index_below\n",
    "\n",
    "\n",
    "def compute_trend(df, columns, interval=3600):\n",
    "    df = df.sort_values('datetime')\n",
    "    for z in df.z.unique():  \n",
    "        this_level = df[df.z == z]\n",
    "        index_above, index_below = make_index(this_level.datetime.values, interval)\n",
    "\n",
    "        for col in columns:\n",
    "            val_above = this_level[col].values\n",
    "            val_below = this_level.iloc[index_below][col].values\n",
    "\n",
    "            time_above = this_level.datetime.values\n",
    "            time_below = this_level.iloc[index_below].datetime.values\n",
    "\n",
    "            trend = 3600 * (val_above - val_below) / (time_above - time_below)\n",
    "\n",
    "            df.loc[df.z == z, col + '_trend'] = trend\n",
    "\n",
    "    return df, [col + '_trend' for col in columns]\n",
    "\n",
    "\n",
    "def get_features(df, use_trend, feature_level):\n",
    "    wind_temp_levels = df.pivot_table(\n",
    "        values=['wind', 'temp'], columns='z', index=['ds', 'tt']\n",
    "    ).reset_index()\n",
    "    wind_temp_levels.columns = [\n",
    "        '%s_%d' % (a, b) if b else a\n",
    "        for a, b in wind_temp_levels.columns.values\n",
    "    ]\n",
    "\n",
    "    df = df.merge(wind_temp_levels, on=['ds', 'tt'])\n",
    "\n",
    "    feature_sets = [\n",
    "        [\n",
    "            'z', 'wind', 'temp', 'soil_temp',\n",
    "            'wind_10', 'wind_20', 'wind_40',\n",
    "            'temp_10', 'temp_20', 'temp_40',\n",
    "        ],\n",
    "        ['soilheat'],\n",
    "        ['netrad'],\n",
    "        ['rain', 'dewpoint'],\n",
    "        ['H', 'LE'],\n",
    "    ]\n",
    "\n",
    "    features = [\n",
    "        f for fset in feature_sets[:feature_level]\n",
    "        for f in fset\n",
    "    ]\n",
    "    \n",
    "    if use_trend:\n",
    "        df, added_cols = compute_trend(df, [\n",
    "            f for f in features if f != 'z'\n",
    "        ])\n",
    "        features.extend(added_cols)\n",
    "\n",
    "    return df, features\n",
    "\n",
    "\n",
    "def get_train_test_data(df, features, target, samples_count, n_months=12):\n",
    "    df = df.dropna()\n",
    "\n",
    "    # get random test months\n",
    "    test_ds = np.random.choice(df.ds.unique(), n_months, replace=False)\n",
    "    test_mask = df.ds.isin(test_ds)\n",
    "    \n",
    "    train_df, test_df = df.loc[~test_mask], df.loc[test_mask]\n",
    "    if samples_count > 0:\n",
    "        # maintain proportion of train/test samples\n",
    "        test_size = int(samples_count * len(test_df) / len(train_df))\n",
    "        train_df = train_df.sample(samples_count)\n",
    "        test_df = test_df.sample(test_size)\n",
    "    \n",
    "    train_x, train_y = train_df[features], train_df[target]\n",
    "    test_x, test_y = test_df[features], test_df[target]\n",
    "\n",
    "    mean_x, mean_y = train_x.mean(), train_y.mean()\n",
    "    std_x, std_y = train_x.std(), train_y.std()\n",
    "\n",
    "    train_x = (train_x - mean_x) /  std_x\n",
    "    test_x = (test_x - mean_x) / std_x\n",
    "    \n",
    "    assert np.all(np.isfinite(train_x))\n",
    "    \n",
    "    train_y = (train_y - mean_y) / std_y\n",
    "    test_y = (test_y - mean_y) / std_y\n",
    "\n",
    "    return train_x, train_y, test_x, test_y, mean_y, std_y\n",
    "    \n",
    "\n",
    "def compute_denormalized_mse(std_y):\n",
    "    def denormalized_mse(y_true, y_pred):\n",
    "        # model is trained with normalized data, but we want\n",
    "        # mse on not normalized data to compare with MOST\n",
    "        mse = K.mean(K.square(y_true - y_pred), axis=-1)\n",
    "        return mse * std_y**2\n",
    "    return denormalized_mse\n",
    "\n",
    "\n",
    "def build_model(input_shape, architecture, std_y=1):    \n",
    "    regularizer = None\n",
    "    layers = [Input(shape=(input_shape,))]\n",
    "\n",
    "    for layer_type, layer_arg in architecture:\n",
    "        if layer_type == 0 or layer_type == 1:\n",
    "            num = 2**layer_arg\n",
    "            layers.append(PReLU()(\n",
    "                BatchNormalization()(\n",
    "                    Dense(num, kernel_initializer=VarianceScaling(2, 'fan_in'),\n",
    "                          kernel_regularizer=regularizer)(\n",
    "                        layers[-1]\n",
    "                    )\n",
    "                )\n",
    "            ))\n",
    "        elif layer_type == 2:\n",
    "            pkeep = (layer_arg + 1) / 10\n",
    "            layers.append(Dropout(pkeep)(layers[-1]))\n",
    "        elif layer_type == 3:\n",
    "            regu = 6 ** -layer_arg\n",
    "            regularizer = regularizers.l2(regu)\n",
    "        else:\n",
    "            raise ValueError('layer type from 0 to 3')\n",
    "\n",
    "    layers.append(Dense(1)(layers[-1]))\n",
    "\n",
    "    opt = Adam(lr=0.001)\n",
    "    model = Model(inputs=layers[0], outputs=layers[-1])\n",
    "    model.compile(loss='mse', optimizer=opt, metrics=[compute_denormalized_mse(std_y)])\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_architecture(step, arch_idx, architecture, max_epochs, samples_count):\n",
    "    train_x, train_y, test_x, test_y, _, std_y = get_train_test_data(\n",
    "        ddf, features, 'phi_m', samples_count, n_months=12\n",
    "    )\n",
    "    \n",
    "    #K.clear_session()  # https://stackoverflow.com/q/35114376/521776\n",
    "    model = build_model(train_x.shape[1], architecture, std_y=std_y)\n",
    "\n",
    "    logdir = 'dev/logs/nas/step-%d-arch-%d' % (step, arch_idx)\n",
    "    callbacks = [\n",
    "        ReduceLROnPlateau(factor=0.1, verbose=0, min_lr=1e-6, patience=10),\n",
    "        TensorBoard(logdir, write_graph=True, write_grads=True, histogram_freq=0),\n",
    "        EarlyStopping(min_delta=0.001, patience=25),\n",
    "    ]\n",
    "\n",
    "    hist = model.fit(\n",
    "        train_x, train_y,\n",
    "        batch_size=1024,\n",
    "        epochs=max_epochs,\n",
    "        verbose=0,\n",
    "        shuffle=True,\n",
    "        callbacks=callbacks,\n",
    "        validation_data=(test_x, test_y)\n",
    "    )\n",
    "\n",
    "    best = min(hist.history['val_denormalized_mse'])\n",
    "\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/ipykernel_launcher.py:211: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    dframe_path = 'data/cabauw/processed-full-log.csv.gz'\n",
    "    df = pd.read_csv(dframe_path, na_values='--', compression='gzip')\n",
    "\n",
    "    df = df[(df.ustar > 0.1) & (abs(df.H) > 10) & (df.wind > 1)]\n",
    "    df = df[df.ds != 201603]\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = load_data()\n",
    "ddf, features = get_features(df, use_trend=True, feature_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3.6/site-packages/tensorflow/python/ops/clip_ops.py:113: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "controller = ControllerRNN(\n",
    "    hidden_size=64,\n",
    "    max_len=20,\n",
    "    batch_size=1,\n",
    "    type_size=3,\n",
    "    arg_size=10,\n",
    "    learning_rate=0.001,\n",
    "    baseline_smoothing=0.99,\n",
    ")\n",
    "\n",
    "hist = []\n",
    "controller_graph = tf.Graph()\n",
    "with controller_graph.as_default():\n",
    "    controller.build()\n",
    "    controller_session = tf.Session(graph=controller_graph)\n",
    "    controller_session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contro_loss=103.844  inner_mse=1.435  inner_mse_std=0.000  length=20.000  fc_count=0.000  dropo_count=1.000  regu_count=19.000  step=0.000  time=23.800\n"
     ]
    }
   ],
   "source": [
    "averages = MovingAverages()\n",
    "averages.smoothing['time'] = 0\n",
    "start_time = time.time()\n",
    "for step_idx in range(10000):\n",
    "    avg_mse = averages.metrics.get('inner_mse', 10)\n",
    "    if avg_mse > 1:\n",
    "        samples_count, max_epochs = 250000, 5\n",
    "    elif avg_mse > 0.5:\n",
    "        samples_count, max_epochs = 500000, 20\n",
    "    elif avg_mse > 0.3:\n",
    "        samples_count, max_epochs = -1, 50\n",
    "    else:\n",
    "        samples_count, max_epochs = -1, 500\n",
    "\n",
    "    architectures = controller.generate_architecture(controller_session)\n",
    "\n",
    "    # test architectures on a temporary graph\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.Session().as_default():\n",
    "            rewards = [\n",
    "                evaluate_architecture(\n",
    "                    step_idx, arch_idx, arch,\n",
    "                    max_epochs, samples_count\n",
    "                )\n",
    "                for arch_idx, arch in enumerate(architectures)\n",
    "            ]\n",
    "\n",
    "    loss = controller.learn_from_rewards(\n",
    "        controller_session, architectures, rewards\n",
    "    )\n",
    "\n",
    "    averages.update_all(\n",
    "        contro_loss=loss**2,\n",
    "        inner_mse=np.mean(rewards),\n",
    "        inner_mse_std=np.std(rewards),\n",
    "        length=np.mean(list(map(len, architectures))),\n",
    "        fc_count=np.mean([sum(1 for lt, _ in arch if lt <= 1) for arch in architectures]),\n",
    "        dropo_count=np.mean([sum(1 for lt, _ in arch if lt == 2) for arch in architectures]),\n",
    "        regu_count=np.mean([sum(1 for lt, _ in arch if lt == 3) for arch in architectures]),\n",
    "    )\n",
    "\n",
    "    if step_idx % 1 == 0:\n",
    "        snap = averages.snapshot(step=step_idx, time=time.time() - start_time)\n",
    "        print('  '.join(\n",
    "            '%s=%.3f' % metric for metric in snap.items()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
